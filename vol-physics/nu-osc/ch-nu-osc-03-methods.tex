\section{Sensitivity Methods}
%{\it Assigned to:} {\bf Elizabeth Worcester} with contributions from Chris Backhouse and Callum Wilkinson. 
\label{sec:physics-lbnosc-sens}



Sensitivities to the neutrino mass ordering, CP violation, and $\theta_{23}$ octant, as well as expected resolution for neutrino oscillation parameter measurements, are obtained by simultaneously fitting the \numutonumu, $\bar{\nu}_\mu \rightarrow \bar{\nu}_\mu$, \numutonue, and $\bar{\nu}_\mu \rightarrow \bar{\nu}_e$ far detector spectra along with selected samples from the near detector.  It is assumed that 50\% of the total exposure is in neutrino beam mode and 50\% in antineutrino beam mode.  A 50\%/50\% ratio of neutrino to antineutrino data has been shown to produce a nearly optimal \deltacp and mass ordering sensitivity, and small deviations from this (e.g., 40\%/60\%, 60\%/40\%) produce negligible changes in these sensitivities. %\todo{check this is still the case} 

In the sensitivity calculations, neutrino oscillation parameters governing long-baseline neutrino oscillation are allowed to vary. In all sensitivities presented here (unless otherwise noted) \sinstt{13} is constrained by a Gaussian prior with 1$\sigma$ width as given by the relative uncertainty shown in Table~\ref{tab:oscpar_nufit}, while \sinst{23}, $\Delta m^{2}_{32}$, and \deltacp are allowed to vary freely. The oscillation parameters $\theta_{12}$ and \dm{12} are allowed to vary constrained by the uncertainty in in Table~\ref{tab:oscpar_nufit}. The matter density of the earth is allowed to vary constrained by a 2\% uncertainty on its nominal value. Systematic uncertainty constraints from the near detector are included either by explicit inclusion of \dword{nd} samples within the fit or by applying constraints expected from the \dword{nd} data to \dword{fd}-only fits.

The experimental sensitivity is quantified using a test statistic, $\Delta\chi^2$, which is calculated by comparing the predicted spectra for alternate hypotheses.  The details of the sensitivity calculations are described in Section~\ref{sect:methods-dunefits}. 
A ``typical experiment'' is defined as one with the most probable data given a set of input parameters, i.e., in which no statistical fluctuations have been applied. In this case, the predicted spectra and the true spectra are identical; for the example of \dword{cpv}, $\chi^2_{\mdeltacp^{true}}$ is identically zero and the $\Delta\chi^2_{CP}$ value for a typical experiment is given by $\chi^2_{\mdeltacp^{test}}$. The interpretation of $\sqrt{\Delta\chi^2}$ has been discussed in \cite{Qian:2012zn,Blennow:2013oma}; it may be interpreted as approximately equivalent to significance in $\sigma$ for $\Delta\chi^2>1$. 



\dword{dune} sensitivity has been studied using several different fitting frameworks. \dword{globes}~\cite{Huber:2004ka,Huber:2007ji} 
-based fits have been used extensively in the past, in particular for sensitivity studies presented in the \dword{dune} \dword{cdr}; details are available in \cite{Acciarri:2015uup,Alion:2016uaj,Bass:2014vta}. \dword{globes} is now used primarily for studies in support of algorithm development and optimization. \dword{valor}\cite{valorweb} has also been used for internal studies.  The sensitivities presented in this document are calculated using the CAFAna analysis framework described below. 



\subsection{The \dword{dune} Analysis Framework}
\label{sect:methods-cafana}

To demonstrate the sensitivity reach of \dword{dune}, we have adopted the analysis framework known as \dword{cafana}~\cite{CAFAna}. This framework was developed for the \dword{nova} experiment and has been used for $\nu_\mu$-disappearance, $\nu_e$-appearance, and joint fits, plus sterile neutrino searches and cross-section analyses.  Unless otherwise noted, sensitivity results presented in this document are performed within \dword{cafana}. %Section~\ref{sect:methods-dunefits} describes the detailed implementation of \dword{dune} sensitivity calculations within the \dword{cafana} analysis framework.

In the sensitivity studies, the compatibility of a particular oscillation hypothesis with the data is evaluated using the likelihood appropriate for Poisson-distributed data \cite{Tanabashi:2018oca}:

\begin{equation}
    \chi^2 = -2\log\mathcal{L} = 2\sum_i^{N_{\rm bins}}\left[ M_i-D_i+D_i\ln\left({D_i\over M_i}\right) \right]
\end{equation}

where $M_i$ is the \dword{mc} %Monte Carlo 
expectation in bin $i$ and $D_i$ is the observed count. Most often the bins here represent reconstructed neutrino energy, but other observables, such as reconstructed kinematic variables or event classification likelihoods may also be used. Multiple samples with different selections can be fit simultaneously, as can multi-dimensional distributions of reconstructed variables.

Event records representing the reconstructed properties of neutrino interactions and, in the case of  \dword{mc}, the true neutrino properties are processed to fill the required histograms. Oscillated \dword{fd} predictions are created by populating \twod histograms, with the second axis being the true neutrino energy, for each oscillation channel ($\nu_\alpha\to\nu_\beta$). These are then reweighted as a function of the true energy axis according to an exact calculation of the oscillation weight at the bin center %\todo{did joao ever write up PMNSOpt?} 
and summed to yield the total oscillated prediction:

\begin{equation}
    M_i = \sum_\alpha^{e,\mu}\sum_\beta^{e,\mu,\tau}\sum_j P_{\alpha\beta}(E_j)M_{ij}^{\alpha\beta}
    \label{eqn:cafana_ll}
\end{equation}

where $P_{\alpha\beta}(E)$ is the probability for a neutrino created in flavor state $\alpha$ to be found in flavor state $\beta$ at the \dword{fd}. $M_{ij}^{\alpha\beta}$ represents the number of selected events in bin $i$ of the reconstructed variable with true energy $E_j$, taken from a simulation where neutrinos of flavor $\alpha$ from the beam have been replaced by equivalent neutrinos in flavor $\beta$. Oscillation parameters that are not displayed in a given figure are profiled over using {\sc minuit}~\cite{James:1994vla}. That is, their values are set to those that produce the best match with the simulated data at each point in displayed parameter space.

Systematic uncertainties are included to account for the expected uncertainties in the beam flux, neutrino interaction, and detector response models used in the simulation at the time of the analysis. The neutrino interaction systematic uncertainties expand upon the existing GENIE systematic uncertainties to include recently exposed data/MC differences that are not expected to be resolved by the time DUNE starts running. The impact of systematic uncertainties is included by adding additional nuisance parameters into the fit. Each of these parameters can have arbitrary effects on the \dword{mc} prediction, and can affect the various samples and channels within each sample in different ways. These parameters are profiled over in the production of the result. The range of these parameters is controlled by the use of Gaussian penalty terms to reflect our prior knowledge of reasonable variations.

For each systematic parameter under consideration, the matrices $M_{ij}^{\alpha\beta}$ are evaluated for a range of values of the parameter, by default $\pm1,2,3\sigma$. The predicted spectrum at any combination of systematic parameters can then be found by interpolation. Cubic interpolation is used, which guarantees continuous and twice-differentiable results, advantageous for gradient-based fitters such as {\sc minuit}. %\todo{worth including a picture of this?}

For many systematic variations, a weight can simply be applied to each event record as it is filled into the appropriate histograms. For others, the event record itself is modified, and for a few systematic uncertainties it is necessary to use an entirely separate sample that has been simulated with some alteration made to the simulation parameters. %\fixme{ CW: should we mention that it uses bin splines anywhere? This may be a hidden weakness of what we do, and we should be honest about that...} EW: We can reconsider this one after this draft.







\subsection{\dword{dune} Sensitivity Studies}
\label{sect:methods-dunefits}
%\fixme{Expand section as results are produced}

\dword{dune} sensitivity studies are performed using the \dword{cafana} framework, which works as described in the previous section. Sensitivity calculations for \dword{cpv}, neutrino mass ordering, and octant are performed, in addition to studies of oscillation parameter resolution in one and two dimensions.
The experimental sensitivity and resolution functions are quantified using a test statistic, $\Delta\chi^2$, which is calculated by comparing the predicted spectra for alternate hypotheses. These quantities are defined for neutrino mass ordering, $\theta_{23}$ octant, and \dword{cpv} sensitivity as follows:

\begin{eqnarray}
\Delta\chi^2_{\textrm{ordering}} & = & \chi^2_{\textrm{opposite}} - \chi^2_{\textrm{true}} \label{eq:dx2_MH}\\
\Delta\chi^2_{\textrm{octant}} & = & \chi^2_{\textrm{opposite}} - \chi^2_{\textrm{either}} \\
\Delta\chi^2_{\textrm{CPV}} & = & \textrm{Min}[\Delta\chi^2_{CP}(\mdeltacp^{\textrm{test}}=0),\Delta\chi^2_{CP}(\mdeltacp^{\textrm{test}}=\pi)],
\end{eqnarray}
where $\Delta\chi^2_{CP} = \chi^{2}_{\mdeltacp^{test}} - \chi^2_{\mdeltacp^{true}}$, and $\chi^2$ is defined in Equation~\ref{eqn:cafana_ll}. Where appropriate, a scan is performed over all possible values of $\mdeltacp^{true}$, and the neutrino mass ordering and the $\theta_{23}$ octant are also assumed to be unknown and are free parameters. The lowest value of $\Delta\chi^2$ is obtained by finding the combination of fit parameters that best describe the simulated data. The size of $\Delta\chi^2$ is a measure of how well those data can exclude this alternate hypothesis given the uncertainty in the model. 



The expected resolution for oscillation parameters is determined from the spread in best-fit values obtained from an ensemble of data sets that vary both statistically and systematically.  For each data set, the true value of each nuisance parameter is chosen randomly from a distribution determined by the a priori uncertainty on the parameter. For some studies, oscillation parameters are also randomly chosen as described in Table~\ref{table:OA_throw}. Poisson fluctuations are then applied to all analysis bins, based on the mean event count for each bin after the systematic adjustments have been applied.  For each simulated data set in the ensemble, the test statistic is minimized, and the best-fit value of all parameters is determined. When calculating $\Delta \chi^{2}$ values from Equation~\ref{eq:dx2_MH}, both of the individual $\chi^{2}$ values used are calculated with the same data set. The one-sigma resolution is defined as the width of the interval around the true value containing 68\% of simulated data sets.
An alternative method of determining parameter resolutions, namely by identifying the range of parameters satisfying $\Delta\chi^2<1$, is also used for some studies.

\begin{dunetable}
[Oscillation parameter throws]
{ccc}
{table:OA_throw}
{Treatment of the oscillation parameters for the simulated data set studies. The width of the $\theta_{13}$ range is determined from the \dword{nufit} result.}
Parameter & Prior & Range\\ \toprowrule
$\sin^{2}\theta_{23}$ & Uniform & [0.4; 0.6] \\
$|\Delta m^{2}_{32}|$ ($\times 10^{-3}$ eV$^{2}$) & Uniform & |[2.3;2.7]| \\
\deltacp ($\pi$) & Uniform & [-1;1] \\
$\theta_{13}$ & Gaussian & \dword{nufit} \\
\end{dunetable}




The \dword{dune} oscillation sensitivities presented here include four \dword{fd} \dword{cc} samples binned as a function of reconstructed neutrino energy: \numutonumu, \numubartonumubar, \numutonue, and \numubartonuebar. Systematic parameters are constrained by unoscillated \dword{nd} \numu and \anumu \dword{cc} samples selected from the \dword{lar} \dword{tpc} and binned in two dimensions as a function of reconstructed neutrino energy ($E_{\nu}$) and reconstructed Bjorken $y$ (i.e. inelasticity). 


For some systematic uncertainties, such as uncertainties on the neutrino flux (Section \ref{sec:nu-osc-05}), the natural treatment leads to a large number of parameters that have strongly-correlated effects on the predicted spectrum. In this case, \dword{pca} is used to create a greatly reduced set of systematic parameters which cover the vast majority of the allowed variation, and remove degenerate parameters. The flux \dword{pca}  is described in Section~\ref{sec:fluxPCA}.

Information from the \dword{nd}, which is used to constrain systematic uncertainties, is included via additional $\chi^2$ contributions (Equation \ref{eqn:cafana_ll}) without oscillations. Specific \dword{nd} samples such as neutrino-electron elastic scattering and off-axis samples may be included separately. %\todo{is this enough to foreshadow DUNE Prism?} 
External constraints, for example from solar neutrino experiments, can be included as an arbitrary term in the $\chi^2$ depending on the oscillation parameters. In practice, a quadratic term, corresponding to a Gaussian likelihood, is used.

\subsubsection{Covariance matrix for \dword{nd} uncertainties}

Far detector energy scale and resolution uncertainties are treated as nuisance parameters in the oscillation fits. These parameters are allowed to vary, and in practice become very weakly constrained in Asimov fits due to the limited statistics of the \dword{fd}. Detector uncertainties in the \dword{nd}, in contrast, are included by adding a covariance matrix to the $\chi^{2}$ calculation.  This choice protects against overconstraining that could occur given the limitations of the parameterized \dword{nd} reconstruction described in Section~\ref{sec:ndsimreco} taken together with the high statistical power at the \dword{nd}.  This covariance matrix is constructed with a many-universes technique. In each universe, all \dword{nd} energy scale, resolution, and acceptance parameters are simultaneously thrown according to their respective uncertainties. The resulting spectra, in the same binning as is used in the oscillation sensitivity analysis, are compared with the nominal prediction to determine the bin-to-bin covariance.



\subsubsection{Implementation of flux uncertainties}
\label{sec:fluxPCA}

% maybe this paragraph is not necessary?
Uncertainties on the flux prediction are described by a covariance matrix, where each bin corresponds to an energy range of a particular beam mode, neutrino species, and detector location. The covariance matrix includes all beam focusing uncertainties evaluated by reproducing the simulation many times, each with simultaneous random variations in the underlying hadron production model. Each random model variation is referred to as a universe. The  matrix used is $208 \times 208$ bins, despite having only $\sim$30 input uncertainties (and thus $\sim$30 significant eigenvalues). To evaluate the impact of these uncertainties on the long-baseline oscillation sensitivity, it is possible to include each focusing parameter, and each hadron production universe, as separate nuisance parameters. It is also possible to treat each bin of the prediction as a separate nuisance parameter, and include the covariance matrix in the log-likelihood calculation. However, both of these options are computationally expensive, and would include many nuisance parameters with essentially no impact on any distributions.

Instead, a principal component analysis is used, primarily to improve the computational performance of the analysis by reducing the number of parameters while still capturing the same physical effects. The covariance matrix is diagonalized, and each principal component is treated as an uncorrelated nuisance parameter. The 208 principal components are ordered by the magnitude of their corresponding eigenvalues, and only the first $\sim$30 are large enough that they need to be included. By the 10th principal component, the eigenvalue is 1\% of the 0th eigenvalue. Since the time required to perform a fit scales $\sim$linearly with the number of nuisance parameters, including only 30 principal components reduces the computing time by an order of magnitude.

This is purely a mathematical transformation; the same effects are described by the \dword{pca} as by a full analysis, including correlations between energy bins. As expected, the largest uncertainties correspond to the largest principal components. This can be seen in Figure~\ref{fig:fluxPCA}. The largest principal component matches the hadron production uncertainty on nucleon-nucleus interactions in a phase space region not covered by data (N+A unconstrained). Components 3 and 7 correspond to the data-constrained uncertainty on proton interactions in the target producing pions and kaons, respectively. Components 5 and 11 correspond to two of the largest focusing uncertainties, the density of the target and the horn current, respectively. Other components not shown either do not fit a single uncertain parameter and may represent two or more degenerate systematics or ones that produce anticorrelations in neighboring energy bins.

\begin{dunefigure}[Flux principal components]{fig:fluxPCA}
{Select flux principal components are compared to specific underlying uncertainties from the hadron production and beam focusing models. See text. }
    \includegraphics[width=0.9\textwidth]{Flux_Unc_PCA.pdf}
\end{dunefigure}



