\chapter{Tools and Methods}
\label{ch:tools}

Evaluation of the capabilities of DUNE/LBNF to realize the scientific program envisioned requires a detailed understanding of the experimental signatures of the relevant physical processes, the response of detection elements, and the performance of calibration systems and event reconstruction and other tools that enable analysis of data from the DUNE detectors.  It is the aim of this chapter to introduce the network of calibration, simulation, and reconstruction tools that form the basis for the demonstration of science capabilities presented in the chapters that follow.  The presentation here covers general components, namely those that are commonly utilized across the science program, although many of these are geared toward application to the long-baseline oscillation physics at the heart of this program.  Other tools and methods developed for specific physics applications are described in the corresponding chapters that follow.

Where appropriate, the performance of reconstruction tools and algorithms is quantified.  Some of these characterizations form the basis for parameterized-response simulations used by physics sensitivity studies that have not yet advanced to the level of analysis of fully reconstructed simulated data.  They also serve as metrics that allow linkages to be drawn between detector configuration specifications and physics sensitivity.

Another critical role for the simulation and reconstruction tools described in this chapter, implicit above, is to enable detailed study of sources of systematic error that can affect physics capability, which can also lead to the development of mitigation strategies.  Thus, where possible, assessments of systematic uncertainties in the modeling of LBNF/DUNE conditions and performance are presented.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Monte Carlo Simulations}
\label{sec:tools-mc}

%Liquid argon time projection chambers (LArTPCs) provide a robust and elegant method for measuring the properties of neutrino interactions above a few tens of MeV by providing 3D event imaging with excellent spatial and energy resolution. For simulations, we produce large samples of Monte Carlo events (Monte Carlo Challenge - MCC) on a regular basis. The purpose is to test the latest simulation and reconstruction chain as well as the grid job submission tools and sam/tape interfaces and provide standard samples for various physics working groups to analyze. All the MCC samples are produced using the batch tool larbatch~\cite{ref:larbatch}.

%MCC1.0 was produced in January 2015 with three 35t samples. We include far detector and protoDUNE samples in later MCCs and incorporate more sophisticated detector simulation and reconstruction software. A more recent MC production MCC6.0, which was produced in May 2016, consists of 50 samples using the 35t, far detector and protoDUNE single-phase geometries. MCC6.0 was produced using LArSoft v05\_09\_01. The locations of the samples can be found in Ref~\cite{ref:mcc6}. 



Many physics processes are simulated in the DUNE \dword{fd}; these include the interactions of beam neutrinos, atmospheric neutrinos, \dword{snb} neutrinos, proton decays and cosmogenic events. Figure~\ref{fig:dune_tpc} shows a portion of the DUNE \single TPC consisting of \dword{apa}s and \dword{cpa}s.

\begin{dunefigure}[Schematic view of a DUNE \single TPC module]{fig:dune_tpc}{A portion of DUNE \single TPC is shown. Four separate drift regions are separated by \dword{apa}s and \dword{cpa}s.}
\includegraphics[width=0.7\textwidth]{graphics/dune_sp_fd.jpg}
\end{dunefigure}

%In MCC6.0, we simulated 3 types of beam neutrinos (unoscillated $\nu_\mu$'s, fully-oscillated $\nu_{e}$'s and fully-oscillated $\nu_\tau$'s), atmospheric neutrinos, supernova neutrinos, proton decays and cosmogenic events in the far detector. 
To save processing time, all the \dword{fd} samples except the cosmogenics sample were simulated using a smaller version of the full \nominalmodsize far \dword{detmodule} geometry. This geometry is \SI{13.9}{m} long, \SI{12}{m} high and \SI{13.3}{m} wide, which consists of 12 \dword{apa}s and 24 \dword{cpa}s. % anne fixed TPCs. 
%In the default configuration, the TPC wire spacing is 5 mm, the wire angle is 36$^{\circ}$ and the neutrino beam is parallel to the wire planes. 
Figure~\ref{fig:dune_apa} shows the detailed structure of an \dword{apa}. %We also generated special samples where the wire spacing is 3 mm or the wire angle is 45$^{\circ}$ or the neutrino beam is perpendicular to the wire planes for the detector optimization studies. 
\begin{dunefigure}
[Detailed structure of the APA]
{fig:dune_apa}
{The detailed structure of the \dword{apa} is shown. Each \dword{apa} consists of four wrapped induction wire planes and two collection wire planes.
The \dword{pd} is sandwiched between the two collection wire planes.}
\includegraphics[width=0.7\textwidth]{DUNE_APA.PNG}
\end{dunefigure}


For the simulation chain, each sample is simulated in three steps: generation (gen), {\sc geant4} tracking (g4), TPC signal simulation, and digitization (detsim). The first step is unique for each sample while the second and the third steps are mostly identical for all samples. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Neutrino Flux Modeling}
\label{sec:tools-mc-flux}
%{\it Assigned to:} {\bf Laura Fields} with contributions from Zarko Pavlovic and Luke Pickering.

Neutrino fluxes were generated using G4LBNF, a \textsc{Geant}4\xspace-based simulation of the LBNF neutrino beam.  The simulation was configured to use a detailed description of the LBNF optimized beam design~\cite{optimizedbeamcdr}.  That design starts with a \SI{1.2}{MW}, \SI{120}{\GeV} primary proton beam that impinges on a \SI{2.2}{m} long, \SI{16}{mm} diameter cylindrical graphite target.  Hadrons produced in the target are focused by three magnetic horns operated with \SI{300}{kA} currents.  The target chase is followed by a \SI{194}{m} helium-filled decay pipe and a hadron absorber.  The focusing horns can be operated in forward or reverse current configurations, creating neutrino and antineutrino beams, respectively.   

\begin{dunefigure}[Visualization of the focusing system as simulated in g4lbnf]{fig:beam_vis}
{Visualization of the focusing system as simulated in g4lbnf.}
    \includegraphics[width=0.7\textwidth]{Beamline_optimized_sept2017.pdf}\end{dunefigure}


The optimized LBNF neutrino beam design is the result of several years of effort by LBNF and DUNE to identify a focusing system optimized to DUNE's long-baseline physics goals.  The optimization process requires scanning many parameters describing the hadron production target, focusing horns, and the decay pipe. Genetic algorithms have been used successfully in the past to scan the large parameter space to find the optimal beam design~\cite{Calviani:2014cxa}. The  LBNF beam optimization process began with a genetic algorithm that scanned simulations of many different horn and target geometries to identify those that produced the optimal sensitivity to \dword{cpv}.  The specific metric used was estimated sensitivity to 75\% of \dword{cp} phase space after \ktmwyr{300}  %kT MW years 
of exposure, taking into account the number and neutrino spectra of all neutrino flavors. The resulting beam effectively optimized flux at the first and second oscillation maxima, which also benefits measurements of other oscillation parameters.  The output of the genetic algorithm was a simple design including horn conductor and target shapes.  This design was transformed into a detailed conceptual design by LBNF engineers, and iterated with DUNE physicists to ensure that engineering changes had minimal impact on physics performance.  Relative to the previous NuMI-like design, the optimized design reduces the time to three-sigma coverage of 75\% of \dword{cp} phase space by 42\%, which is equivalent to increasing the mass of the far detector by 70\%.  It also substantially increases sensitivity to the mass hierarchy and improves projected resolution to quantities such as $\sin^22\theta_{13}$ and $\sin^2\theta_{23}$~\cite{fields_doc_2901}.        

\subsubsection{On-axis Neutrino Flux and Uncertainties}

%\fixme{in figure 4, near or far? the captions don't match. anne}
%ZP: It is near. Fixed the caption
\begin{dunefigure}[Neutrino fluxes at the near detector]{fig:flux_flavor_parent}
{Predicted neutrino fluxes at the near detector for neutrino mode (left) and antineutrino mode (right). From top to bottom shown are muon neutrino, muon antineutrino, electron neutrino, and electron antineutrino fluxes.}
\includegraphics[width=0.9\textwidth]{graphics/ND_HadronParentFluxComponents_0m_offaxis.pdf}
%    \includegraphics[width=0.35\textwidth]{graphics/dune_neutrino_nd_numu.pdf}
%     \includegraphics[width=0.35\textwidth]{graphics/dune_antineutrino_nd_numu.pdf}
%    \includegraphics[width=0.35\textwidth]{graphics/dune_neutrino_nd_numubar.pdf}
%     \includegraphics[width=0.35\textwidth]{graphics/dune_antineutrino_nd_numubar.pdf}
%    \includegraphics[width=0.35\textwidth]{graphics/dune_neutrino_nd_nue.pdf}
%     \includegraphics[width=0.35\textwidth]{graphics/dune_antineutrino_nd_nue.pdf}
%    \includegraphics[width=0.35\textwidth]{graphics/dune_neutrino_nd_nuebar.pdf}
%     \includegraphics[width=0.35\textwidth]{graphics/dune_antineutrino_nd_nuebar.pdf}
\end{dunefigure}

%\todo{Add numbers for flavor composition of beam}
%zp: added fractions for optimized beam at FD
%zp: updated fig:flux_flavor to Optimized 
%zp: changed it to fig:flux_flavor_parent since fig:flux_flavor label used in another volume
The predicted neutrino fluxes for neutrino and antineutrino mode configurations of LBNF are shown in Figure~\ref{fig:flux_flavor_parent}.  In neutrino (antineutrino) mode, the beams are 92\% (90.4\%) muon neutrinos (antineutrinos), with wrong-sign contamination making up 7\% (8.6\%) and electron neutrino and antineutrino backgrounds 1\% (1\%).  Although %there is also expected to be 
we expect a small non-zero intrinsic tau neutrino flux, this is not simulated by G4LBNF.  Nor are neutrinos arising from particle decay at rest. % are also not simulated.  

\begin{dunefigure}[Flux uncertainties at the \dword{fd} as a function of neutrino energy]{fig:flux_uncertainties_flavor}
{Flux uncertainties at the far detector as a function of neutrino energy in neutrino mode (left) and antineutrino mode (right) for, from top to bottom, muon neutrinos, muon antineutrinos, electron neutrinos and electron antineutrinos. }
    \includegraphics[width=0.85\textwidth]{graphics/HP_NonHP_FD_allspec_ErrBreakdown.pdf}
%    \includegraphics[width=0.4\textwidth]{error_overlay_numu_neutrino_FD_opt.png}
%    \includegraphics[width=0.4\textwidth]{error_overlay_numu_antineutrino_FD_opt.png}
%    \includegraphics[width=0.4\textwidth]{error_overlay_numubar_neutrino_FD_opt.png}
%    \includegraphics[width=0.4\textwidth]{error_overlay_numubar_antineutrino_FD_opt.png}
%        \includegraphics[width=0.4\textwidth]{error_overlay_nue_neutrino_FD_opt.png}
%    \includegraphics[width=0.4\textwidth]{error_overlay_nue_antineutrino_FD_opt.png}
%        \includegraphics[width=0.4\textwidth]{error_overlay_nuebar_neutrino_FD_opt.png}
%    \includegraphics[width=0.4\textwidth]{error_overlay_nuebar_antineutrino_FD_opt.png}
    \end{dunefigure}

%\fixme{defin NDTF, PPFX. anne}
%ZP: expanded PPFX to Package to Predict the FluX (PPFX) and citation is also given for the package
%ZP: Removed NDTF since it will not be mentioned in final caption, just left comment that plots still need to be updated
%ZP Oct 2019: Updated using the latest plots from Luke P.

\begin{dunefigure}[Focusing and hadron production uncertainties on the $\nu$ mode $\nu_{\mu}$ flux]{fig:flux_uncertainty_breakdown}
{Focusing (left) and hadron production (right) uncertainties on the neutrino mode muon neutrino flux at the \dword{fd}.}
  \includegraphics[width=0.45\textwidth]{graphics/NonHP_FD_numu_ErrBreakdown_left.pdf}
  \includegraphics[width=0.45\textwidth]{graphics/HP_FD_numu_ErrBreakdown_right.pdf}
%ZP Oct 2019: Updated using the latest plots from Luke P.
\end{dunefigure}

Uncertainties on the neutrino fluxes arise primarily from uncertainties in hadrons produced off the target and uncertainties in parameters of the beam such as horn currents and horn and target positioning (commonly called ``focusing uncertainties'').  Uncertainties on the neutrino fluxes arising from both of these categories of sources are shown in Figure~\ref{fig:flux_uncertainties_flavor}.  Hadron production uncertainties are estimated using the Package to Predict the FluX (PPFX) framework developed by the \minerva collaboration~\cite{Aliaga:2016oaz, AliagaSoplin:2016shs}, which assigns uncertainties for each hadronic interaction leading to a neutrino in the beam simulation, with uncertainties taken from thin target data (from e.g., the NA49~\cite{NA49} experiment) where available, and large uncertainties assigned to interactions not covered by data.  Focusing uncertainties are assessed by altering beamline parameters in the simulation within their tolerances and observing the resulting change in predicted flux.  A breakdown of the hadron production and focusing uncertainties into various components are shown in Figure~\ref{fig:flux_uncertainty_breakdown} for the neutrino mode muon neutrino flux at the \dword{fd}.    

At most energies, hadron production uncertainties are dominated by the ``NucleonA'' category, which includes proton and neutron interactions that are not covered by external data.  At low energies, uncertainties due to pion reinteractions (denoted ``Meson Inc'') dominate.   The largest source of focusing uncertainty arises from a 1\% uncertainty in the horn current, followed by a 2\% %flat
uncertainty in the number of protons impinging on the target.   For all neutrino flavors and all neutrino energies, hadron production uncertainties are larger than focusing uncertainties.  However, hadron production uncertainties are expected to decrease in the next decade, as more thin target data becomes available.  Hadron production measurements taken with a replica target are also being considered and would substantially reduce the uncertainties.  

\begin{dunefigure}[Correlation of flux uncertainties]{fig:flux_uncertainty_correlation}
{Correlation of flux uncertainties.  Each block of neutrino flavor corresponds to bins of energy with bin boundaries of
[0.0, 0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.4, 1.6, 1.8, 2.0, 2.2, 2.4, 2.6, 2.8, 3.0, 3.2, 3.4, 3.6, 3.8, 4.0, 4.5, 5.0, 5.5, 6.0, 6.5, 7.0, 8.0, 9.0, 10.0, 12.0, 15.0, 20.0] GeV for right sign muon neutrinos, [0.0, 0.2, 0.4, 0.6, 0.8, 1.0, 1.5, 2.0, 2.5, 3.0, 4.0, 5.0, 6.0, 8.0, 10.0, 20.0] GeV for wrong sign muon neutrinos, and [0.0, 0.75, 1.5, 2.5, 3.5, 5.0, 7.5, 10.0, 20.0] for electron neutrinos and antineutrinos. }
    \includegraphics[width=0.9\textwidth]{graphics/FluxOnAxisCorrMatrix.pdf}
%    \includegraphics[width=0.9\textwidth]{correlation_ndtf_opt.png}
\end{dunefigure}

Figure~\ref{fig:flux_uncertainty_correlation} shows correlations of the total flux uncertainties.  In general, the uncertainties are highly correlated across energy bins. However, the flux in the very high energy, coming predominantly from kaons, tends to be uncorrelated with flux at the peak, %dominated 
arising predominantly from pion decays.  Flux uncertainties are also highly correlated between the near and far detectors and between neutrino-mode and antineutrino-mode running.  The focusing uncertainties do not affect wrong-sign backgrounds, which reduces correlations between e.g., muon neutrinos and muon antineutrinos in the same running configuration in the energy bins where focusing uncertainties are significant.    

%Although the fluxes at the \dword{nd} and \dword{fd} are similar, they are not identical.  
The unoscillated fluxes at the \dword{nd} and \dword{fd} are similar but not identical. Figure~\ref{fig:flux_nearfar} shows the ratio of the near and far neutrino-mode muon neutrino unoscillated fluxes %at the near and far detectors 
and the uncertainties on the ratio.  The uncertainties are approximately 1\% or smaller except at the falling edge of the focusing peak, where they rise to 2\%, but are still much smaller than the uncertainty on the absolute fluxes.    And unlike the case for absolute fluxes, the uncertainty on the near-to-far flux ratio is dominated by focusing rather than hadron production uncertainties.  This ratio and its uncertainty are for the fluxes at the center of the near and far detectors,
%\fixme{which, near or far?}
%ZP: inserted "near and far" since it is for both though this is not so relevant for far.
and do not take into account small variations in flux across the face of the \dword{nd}.     

\begin{dunefigure}[Ratio of neutrino-mode muon neutrino fluxes at the near and far detectors]{fig:flux_nearfar}
{Ratio of neutrino-mode muon neutrino fluxes at the near and far detectors (left) and uncertainties on the ratio (right). }
    \includegraphics[width=0.45\textwidth]{graphics/NDFD_ratio_left.pdf}
     \includegraphics[width=0.45\textwidth]{graphics/HP_NonHP_FDND_Ratio_ErrBreakdown_right.pdf}
%    \includegraphics[width=0.45\textwidth]{nearfar.png}
%     \includegraphics[width=0.45\textwidth]{nearfarunc.png}
%ZP Oct 2019: Updated using the latest plots from Luke P.
\end{dunefigure}

%\fixme{see the 'to be updated in figure}
%ZP Oct 2019: Updated figures using optimized beam flux

\subsubsection{Off-axis Neutrino Flux and Uncertainties}

%The neutrino flux spreads beyond the beam aimed directly at the \dword{fd}, and viable neutrino fluxes extend outward at the \dword{nd} hall.  For an ``off-axis'' angle relative to the initial beam direction, the subsequent neutrino energy spectrum is narrower and peaked at a lower energy than the on-axis spectrum. At \SI{575}{m}, the location of the \dword{nd} hall, a lateral shift of \SI{1}{m} corresponds to approximately a \ang{0.1} change in off-axis angle.   
%A discussion of off-axis flux modeling is given in %the Appendix, in Section~\ref{sec:tools-app-flx-offaxis}.

%\subsubsection{Off-axis Neutrino Flux and Uncertainties}
%\label{sec:tools-app-flx-offaxis}

The neutrino flux has a broad angular distribution and extends outward at the \dword{nd} hall. At an ``off-axis'' angle relative to the initial beam direction, the subsequent neutrino energy spectrum is narrower and peaked at a lower energy than the on-axis spectrum.
The relationship between the parent pion energy and neutrino energy is shown in Figure~\ref{fig:OAAFluxFigs}.  At \SI{575}{m}, the location of the \dword{nd} hall, a lateral shift of \SI{1}{m} corresponds to approximately a \ang{0.1} change in off-axis angle. 
The DUNE-PRISM concept, in which the near detector \lartpc can be moved to enable  off-axis measurements, relies on this feature to help constrain systematic errors for the \dword{lbl} oscillation program as described in Section~\ref{sec:ch-nu-osc-06-ndconcept-offaxis}.

\begin{dunefigure}[$\nu$ energy as  function of parent pion energy for off-axis angles] %different angles away from the pion momentum vector]
{fig:OAAFluxFigs}
{(left) The neutrino energy as a function of parent pion energy for different angles away from the pion momentum vector. Figure from Ref.~\cite{Duffy:2016owt}. (right) The DUNE near detector flux predictions over a range of off-axis positions for a near detector at \SI{575}{m} downstream of the target station. }
%is subfig package included? I don't think subfloat is working (ZP)
%  \subfloat[C][Off-axis pion decay kinematics]{
    \includegraphics[width=0.4\textwidth]{OATrick}
%    \label{fig:OATrick}
%  }
%  \subfloat[][DUNE near detector flux predictions]{
    \raisebox{0.5em}{\includegraphics[width=0.5\textwidth]{graphics/dune_numu_offaxis_flux.pdf}}
%    \label{fig:OffAxisFluxes_1D}
%  }
\end{dunefigure}

The intrinsic neutrino flavor content of the beam varies with off-axis angle. Figure~\ref{fig:OffAxisFluxes_1D_AllSpec} shows the neutrino-mode and anti-neutrino-mode predictions for the four neutrino flavors at the on-axis position, and a moderately off-axis position. At the \SI{30}{m} position, a second, smaller energy peak at approximately \SI{4}{\GeV} is due to the charged kaon neutrino parents. 
%As both the pion and kaon-parent peaks are significantly narrower in observed neutrino energy at greater off-axis angle, this which may allow for off-axis kaon-parent analyses.

%\fixme{Fig~\ref{fig:OffAxisFluxes_1D_AllSpec} is partly redundant with earlier figure, nice if it can be merged? }
%ZP: Changed previous plot. Unlike this one it shows all of the neutrino species broken by parent and it is on linear scale. Here useful to have comparison on vs offaxis and on log scale
\begin{dunefigure}[The predicted ND \numu energy spectra, on axis and \SI{30}{m} off axis]{fig:OffAxisFluxes_1D_AllSpec}
{The predicted muon neutrino energy spectra at two \dword{nd} positions, on axis and \SI{30}{m} off axis. (a) The predicted neutrino flavor-content of the neutrino-mode (FHC) and anti-neutrino-mode (RHC) beam. (b) The neutrino-mode, muon-flavor predicted flux, separated by the particle that decayed to produce the neutrino. The off-axis spectrum displays a double peak structure due to charged kaon parent decay kinematics. The on-axis kaon-peak occurs at higher neutrino energy and will have a significantly broader energy spread. Top: Beam neutrino flavor content, middle: Beam neutrino flavor content; bottom: Beam neutrino decay-parent species}
   % \includegraphics[width=0.8\textwidth]{OffAxisFluxes_1D_AllSpec}
    \includegraphics[width=0.8\textwidth]{OffAxisFluxes_1D_AllSpec}
  \includegraphics[width=0.8\textwidth]{OffAxisFluxes_1D_Parents}    
    \end{dunefigure}

%\fixme{one label per dunefigure; I didn't use label fig:OffAxisFluxes\_1D\_Parents or fig:OAAFluxFigsDetail - may need to fix refs}
%ZP: not sure what was the issue. looks fine to me now

The same sources of systematic uncertainty that affect the on-axis spectra also modify the off-axis spectra. 
Figure \ref{fig:onvsoff_flux_uncertainty} shows the on-axis and off-axis hadron production and focusing uncertainties. 
Generally, the size of the off-axis uncertainties is comparable to the on-axis uncertainties and the uncertainties are highly correlated across off-axis and on-axis positions. While the hadron production uncertainties are similar in size, the focusing uncertainties are smaller for the off-axis flux. The systematic effects have different shapes as a function of neutrino energy at different off-axis locations, making off-axis flux measurements useful to diagnose beamline physics. Measuring on-axis and off-axis flux breaks degeneracy between various systematics and allows better flux constraint.

%\fixme{DISCUSS FIGURE: Plot of extreme off-axis angle 1D uncertainty profiles? Can be relative off-axis to on-axis or just off-axis only. }

\begin{dunefigure}[On-axis and off-axis flux uncertainties]{fig:onvsoff_flux_uncertainty}
{The flux uncertainty for the on-axis flux, and several off-axis positions. Shown is the total hadron production uncertainty and several major focusing uncertainties.}
    \includegraphics[width=0.8\textwidth]{graphics/onaxis_vs_offaxis_uncertainties.png}
\end{dunefigure}

\subsubsection{Alternate Beamline Configurations}

Although the LBNF beamline is expected to run for many years in a \dword{cp}-optimized configuration, it could potentially be modified in the future for other physics goals.  For example, it could be altered to produce a higher-energy spectrum %in order 
to measure tau neutrino appearance.  In the standard \dword{cp}-optimized configuration, we expect about 130 tau neutrino \dword{cc} interactions per year %are expected 
at the \dword{fd}, before detector efficiency and assuming \SI{1.2}{MW} beam power.  However, replacing the three \dword{cp}-optimized horns with two NuMI-like parabolic horns can raise this number to approximately \num{1000} tau neutrinos per year.  Figure~\ref{fig:tau-optimized} shows the muon neutrino flux for one such configuration.  Although the flux in the \SIrange{0}{5}{\GeV} region critical to $\delta_{CP}$ measurements is much smaller, the flux above \SI{5}{\GeV}, where the tau neutrino interaction cross section becomes significant, is much larger.  Many other energy distributions are possible by modifying the position of the targets and horns.  Even altering parameters of the \dword{cp}-optimized horns offers some variablity in energy spectrum, but the parabolic NuMI horns offer more configurability.  Because the LBNF horns are not expected to be remotely movable, such reconfigurations of the beamline would require lengthy downtimes to reconfigure target chase shielding and horn modules.   

\begin{dunefigure}[Comparison of standard and tau-optimized neutrino fluxes]{fig:tau-optimized}
{Comparison of standard and tau-optimized neutrino fluxes.  The tau optimized flux was simulated with a \SI{120}{\GeV} proton beam and two NuMI parabolic horns, with the second horn starting \SI{17.5}{m}
%\fixme{check m unit}
%ZP: horn 2 is at 17.5m in this config
downstream from the start of the first horn, and a \SI{1.5}{m} long, \SI{10}{mm} wide carbon fin target starting \SI{2}{m} from the upstream face of the first horn.  
%To be updated
}
%ZP: Updated plot (made legend bigger)
\includegraphics[width=0.5\textwidth]{tau_vs_nominal_neutrino_fd.pdf}
\end{dunefigure}

%\fixme{figure To be updated}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Neutrino Interaction Generators} 
\label{sec:tools-mc-gen}

%\begin{itemize}
%\item Have this section owned by a Genie expert. Costas?
%\end{itemize}

%Both the beam neutrinos and atmospheric neutrinos were simulated using {\sc genie} v2\_10\_6. The beam neutrinos were simulated using the reference flux. We simulated unoscillated samples in both the neutrino and antineutrino modes. We also simulated fully oscillated samples where we convert all $\nu_\mu$'s in the beam to either $\nu_e$'s or $\nu_\tau$'s. 

%The atmospheric neutrinos were simulated using the Bartol flux for the Soudan site~\cite{ref:bartol}. There is a plan to use the Honda flux for the Homestake site~\cite{ref:honda} when the {\sc genie} flux interface is updated. 

\subsubsection{Supernova Neutrinos}

The \dword{snb} neutrino events were generated using custom code wrapped in a \dword{larsoft} module.
This code simulates \dword{cc} $\nu_e$-$^{40}$Ar interactions.
For each electron neutrino it calculates probabilities to produce a $^{40}$K nucleus
in different excited states (using a model from~\cite{Bhattacharya:1998hc}),
randomly selects one, and (with energy levels from~\cite{Cameron:2004myb}) 
produces several de-excitation $\gamma$s and an electron carrying the remaining energy.
All particles are produced isotropically, and 
there is no delay between the electron and corresponding de-excitation $\gamma$s
(in this model the $^{40}$K nucleus de-excites instantaneously) and they share a vertex,
which is simulated with equal probability anywhere in the active volume.
The primary neutrino energy distribution used in these samples is the cross-section-weighted 
energy spectrum obtained from SNOwGLoBES~\cite{snowglobes} (using the ``GKVM'' flux~\cite{GKVM}).
The \dword{snb} neutrino generator also allows to simulate a Poisson-distributed random number 
of neutrino interactions per event. These samples were simulated with, on average, 2 or 20 neutrinos.
In addition, one of the samples was generated with $1.01$~Bq/kg of $^{39}$Ar background.

\subsubsection{GENIE}

The DUNE \dword{mc} simulation chain is interfaced to the \dword{genie} event generator \cite{Andreopoulos:2009rq}. This is an open-source product of the \dword{genie} collaboration\footnote{www.genie-mc.org}  that provides state-of-the-art modeling of neutrino-nucleus interactions, as well as simulation of several other non-neutrino processes (nucleon decay, neutron-antineutron oscillation, boosted dark matter interactions, hadron and charged lepton scattering off nuclei). The generator product also includes off-the-shelf components (flux drivers and interfaces to outputs of detailed neutrino beamline simulations, detector geometry drivers, and several specialized event generation applications) for the simulation of realistic experimental setups. The \dword{genie} collaboration performs an advanced global analysis of neutrino scattering data, and is leading the development and characterization of comprehensive interaction models. The \dword{genie} comprehensive models and physics tunes which are developed using its proprietary Comparisons and Tuning products, are fully integrated in the \dword{genie} Generator product. Finally, the open-source \dword{genie} Reweight product provides means for propagating modeling uncertainties. 

At the time of the \dword{tdr} writing, the DUNE simulation uses a version in the v2 series of the \dword{genie} Generator, which includes empirical comprehensive models, based on home-grown hadronic simulations (AGKY model \cite{Yang:2009zx} for neutrino-induced hadronization and INTRANUKE/hA model \cite{Dytman:2015taa} for hadronic re-interactions) and nuclear neutrino cross sections calculated within the framework of the simple Relativistic Fermi Gas model \cite{Bodek:1981wr}. Several processes are simulated within that framework with the most important ones, in terms of the size of the corresponding cross section at a few GeV, being: (1) quasi-elastic scattering, simulated using an implementation of the Llewellyn Smith model \cite{LlewellynSmith:1971uhs}, (2) multi-nucleon interactions, simulated with an empirical model motivated by the Lightbody model \cite{Lightbody:1988gcu} and using a nucleon cluster model for the simulation of the hadronic system, (3) baryon resonance neutrino-production simulated using an implementation of the Rein-Sehgal model \cite{Rein:1980wg}, and (4) deep-inelastic scattering, simulated using the model of Bodek and Yang \cite{Bodek:2002ps}.  These comprehensive models, as well as the \dword{genie} procedure for tuning the cross section model in the transition region, have been used for several years and are well understood and documented \cite{Andreopoulos:2009rq}. The actual tune used is the one produced for the analysis of data from the MINOS experiment and, as was already known at that time, it has several caveats as it emphasizes inclusive data and does not address tensions with exclusive data. The future DUNE simulation will be done using the v3 \dword{genie} Generator where improved models and tunes are available. Details of improved models in the v3 \dword{genie} Generator are discussed in \ref{sec:tools-app-generator}. %These comprehensive models and tunes are now unsupported, as they are superseded by the substantially improved versions listed below. However, they are included in v3 as they provide a connection with a large body of neutrino interaction studies.



Besides simulation of neutrino-nucleus interactions, \dword{genie} provides simulation of several \dword{bsm} physics channels:

\textit{\dword{bsm}}: The implementation of a \dword{bsm} \dword{mc} simulation has been motivated by several theory studies \cite{Agashe:2014yua, 
Berger:2014sqa, Kong:2014mia, Cherry:2015oca, Kopp:2015bfa, Necib:2016aez, Alhazmi:2016qcs, Kim:2016zjx}. The current implementation focuses on two models presented in  \cite{Berger:2014sqa}. The first has a fermionic \dword{dm} candidate, a $Z^\prime$ mediator, and velocity independence of the spin-dependent cross section in the non-relativistic limit. The second model has a scalar \dword{dm} candidate, a $Z^\prime$ mediator, and a $u^2$ velocity dependence of the spin-dependent cross section in the non-relativistic limit.

\textit{Nucleon decay}: \dword{genie} simulates several nucleon decay topologies. For the initial nuclear state environment and intranuclear hadron transport, it uses the same modeling as it does for neutrino event simulation. In the nucleon decay simulation, the nucleon binding and momentum distribution is simulated using one of the nuclear models implemented in \dword{genie} (typically a Fermi Gas model), and it is decayed to one of many topologies using a phase space decay. The decay products are produced within the nucleus and further re-interactions of hadrons are simulated by the \dword{genie} hadron transport models. The simulated nucleon decay topologies are given in Table~\ref{tab:genie_ndk}, presented in the Appendix
in Sec.~\ref{sec:tools-app-generator}.

\textit{Neutron-antineutron oscillation}: \dword{genie} simulates several event topologies that may emerge following the annihilation of the antineutron produced from a bound neutron to antineutron transition. For the initial nuclear state environment and intranuclear hadron transport, the simulation, as in the case of nucleon decay, uses the same modeling as it does for the neutrino event simulation. The simulated reactions are listed in Table~\ref{tab:nnbar-br} in 
Sec.~\ref{subsec:nonaccel-nnbar-dunesensitivity}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Detector Simulation}
\label{sec:tools-mc-detsim}

%\begin{itemize}
%\item Charged particle-argon interactions
%\item Radiologicals
%\begin{itemize} \item Table from Jason Stock \end{itemize}
%\item Geometries
%\begin{itemize} \item Figure showing ProtoDUNE, 1x2x6 \end{itemize}
%\item LArG4
%\item Photon Simulation
%\begin{itemize}
%\item LAr optical properties
%\item Photon Library (or replacement) method
%\item Photon library figure
%\end{itemize}
%\item TPC detector signal simulation (Xin et al.)
%\end{itemize}

The detector simulation consists of particle propagation in the liquid argon using {\sc geant} and the TPC and photon detector response simulation. This step is done in the common framework \dword{larsoft} and is validated by other \dword{lartpc} experiments such as ArgoNeuT, MicroBooNE, LArIAT and ProtoDUNE.

\subsubsection{LArG4}\label{sec:larg4}

The truth particles generated in the event generator step are passed to a {\sc geant4} v4\_10\_1\_p03-based detector simulation. In this step, each primary particle from the generator and its decay or interaction daughter particles are tracked when they traverse \lar. The energy deposition is converted to ionization electrons and scintillation photons. Some electrons are recombined with the positive ions~\cite{Acciarri:2013met,Amoruso:2004dy} while the rest of the electrons are drifted towards the wire planes. The number of electrons is further reduced due to the existence of impurities in the \lar, which is commonly parameterized as the electron lifetime. %The default electron lifetime is \SI{3}{ms} in the simulation.
Unless otherwise specified, an electron
lifetime of \SI{3}{ms} is assumed in the simulations.
The longitudinal diffusion smears the arrival time of the electrons at the wires and the transverse diffusion smears the electron location among neighboring wires. More details
regarding the recent measurements of diffusion coefficients can be found
in~\cite{Li:2015rqa,larpropertiesbnl}.

\subsubsection{Photon Simulation}

When ionization is calculated, the amount of scintillation light is also calculated. The response of the \dwords{pd} is simulated using a ``photon library,'' a pre-generated table giving the likelihood that photons produced within a voxel in the detector volume  will reach any of the \dwords{pd}. The photon library is generated using \dword{geant4}'s photon transport simulation, including \SI{66}{cm} Rayleigh scattering length, \SI{20}{m} attenuation length, and reflections off of the interior surface detectors. The library also incorporates the response versus location of the \dwords{pd}, capturing the attenuation between the initial conversion location of the photon and the \dwords{sipm}.

\subsubsection{TPC Detector Signal Simulation}\label{sec:tpc_sim}

When ionization electrons drift through the induction wire planes toward the collection wire plane, current is
induced on nearby wires. The principle of current induction is described by the Ramo theorem~\cite{Shockley1938,Ramo:1939vr}. 
For an element of ionization charge, the instantaneous induced current $i$ is proportional to the amount of drifted charge $q$: 
\begin{equation}\label{eq:shockley_ramo}
  i = - q \cdot \vec{E}_w \cdot \vec{v}_q.
\end{equation}
The proportionality factor is a product of the weighting field $\vec{E}_w$ at the location of the charge and 
the charge's drifting velocity $\vec{v}_q$. The weighting field $\vec{E}_w$ depends on the geometry of 
the electrodes. The charge's drifting velocity $\vec{v}_q$ is a function of the external \efield, which 
also depends on the geometry of the electrodes as well as the applied drifting and bias voltages. The current induced at a given electrode and electron drift path ($x$)
  sampled over a period of time ($t$) is called a ``field response function'' $R(x,t)$.
\begin{dunefigure}
[Garfield configuration for simulating the field response functions]
{field_resp_geometry}
{Garfield configuration for simulating the field response functions.}
\includegraphics[width=0.85\textwidth]{apa_cell.png}
\end{dunefigure}


The field response functions for a single ionization electron are simulated with Garfield~\cite{garfield}. \fixme{add to glossary?} 
In the Garfield simulation, a \SI{22}{cm} (along the \efield or drift direction) $\times$ 
\SI{30}{cm} (perpendicular to the field direction and wire orientation) region is configured.  
Figure~\ref{field_resp_geometry} shows a part of the region close to the anode wire planes. There are five wire planes with \SI{4.71}{mm} spacing, referred to as G, U, V, X, and 
M with operating bias voltages of \SI{-665}{V}, \SI{-370}{V}, \SI{0}{V}, \SI{820}{V}, \SI{0}{V}, respectively.  
These bias voltages ensure 100\% transmission of electrons through the grid plane (G) and the 
first two induction planes (U and V) and complete collection by the collection plane X with the main drift 
field at \SI{500}{V/cm}. In the simulation, each wire plane contains 101 wires with \SI{150}{\micro\meter} diameter
  separated at $\sim\,$\SI{4.71}{\mm} wire pitch. The electron drift velocity as a function of electric
  field is taken from recent measurements~\cite{Li:2015rqa,larpropertiesbnl}. In this simulation, the motion of the positive ions is not included as their drift velocity is about five orders of magnitude slower than that of ionization electrons. In the underground condition, the distortion in E-field caused by the space charge (accumulated positive ions) is expected to be a factor of 100 smaller than that in the surface-operating ProtoDUNE detectors. This leads the maximal position distortion to be less than 3 mm. 
\begin{dunefigure}
[Position-dependent (long-range) field response simulated with the Garfield program]
{field_resp}
{Position-dependent (long-range) field response simulated with the Garfield program 
for two induction and one collection planes.
The $z$-axis scale is logarithmic ($\propto{\rm sgn}(i)\log(|i|))$.
%in the ``Log~10'' format.
The wire of interest 
is assumed to locate at position zero. When a cloud of ionization electrons are drifting through a particular transverse position, the waveform on the wire of interest is shown in z-axis along the x-axis (drift time). Obviously, as the magnitude of transverse positions are large, the induced signal becomes small.}
\includegraphics[width=0.7\textwidth]{field_response_data.png}
\end{dunefigure}


 Given the above configuration, the field response function can then be calculated in Garfield
  for each individual wire  for an
  electron starting from any position within the region of simulation. 
  The field response functions for a range $\pm\,$10 wires on both sides of the central wire (covering 21 wires in total) are 
  recorded %.
  %In addition to this position, five other positions with
  %\SIlist{0.471;0.942;1.413;1.884;2.355}{\mm} horizontal %shift towards one direction are also simulated.
  %In total, 126 field responses (six positions for 21 %readout wires) are calculated for each wire plane
  and stored for later
  application in the TPC detector signal simulation.
  Figure~\ref{field_resp} shows the simulated field response.
  
%  ($i$ in $\mu A$ with 0.1 $\mu s$ time bin) 
%   in the ``Log~10'' format:
%   \begin{equation}\label{eq:log10}
%    i \text{ in ``Log 10'' } =
%    \begin{cases}
%      \text{log}_{10}(i\cdot4\times10^{10}),& \text{if }i>2.5\times10^{-11},\\
%      0,& \text{if }  -2.5 \times10^{-11} \leq i \leq 2.5 \times 10^{-11},\\
%     -\text{log}_{10}(- i\cdot4\times10^{10})& \text{if }i<-2.5\times10^{-11}.\\
%      \end{cases}
%  \end{equation}
  
  Following the earlier work in MicroBooNE~\cite{Adams:2018dra}, the TPC detector signal simulation
  is implemented in the software package Wire-Cell Toolkit~\cite{ref:wire_cell_toolkit,ref:full_simulation}, which is 
  further interfaced with \dword{larsoft}. This simulation procedure has been validated in the MicroBooNE experiment~\cite{Adams:2018gbi}. In
  the following, we summarize the major features. The TPC signal simulation takes input from the \dword{geant4}-simulated energy deposition when particles traverse the detector, and outputs digitized waveforms on the \dword{fe} electronics.
   A data-driven, analytical simulation of the inherent electronics noise is also performed. 
Figure~\ref{fig:simevent} shows the example waveform for minimum ionizing particles traveling parallel to 
the wire plane, but perpendicular to the wire orientation.  
%Figure~\ref{fig:simevent} shows one example event of 
%a \dword{mip} traveling through one \dword{apa}. 

\begin{dunefigure}
[Waveform for minimum ionizing particles traveling parallel to the wire plane]
{fig:simevent}
{Waveform for minimum ionizing particles traveling parallel to the wire plane. For different
        wire plane, the corresponding track is assumed to travel perpendicular to the wire orientation.}
\includegraphics[width=0.6\textwidth]{DUNE_line_source_wf.png}
\end{dunefigure}


The signal simulation, i.e., the \dword{adc} waveform on a given channel, 
\begin{equation}
  \label{eq:sim-convolution}
    M = (Depo \otimes Drift \otimes Duct + Noise) \otimes Digit, 
\end{equation}
%\footnote{symbol $\otimes$: circular convolution, also known as cyclic convolution or periodic convolution, arises in the context of the discrete-time Fourier Transform (DTFT).}
is conceptually a convolution of five functions:
\begin{description}
\item[$Depo$] represents the initial distribution of the ionization electrons created by energy depositions in space and time as
discussed in Section~\ref{sec:larg4}.
\item[$Drift$] represents a function that transforms an initial charge cloud to a distribution of electrons arriving at the wires. Physical processes related to drifting, including attenuation due to impurities, diffusion and possible distortions to the nominal applied \efield, are applied in this function.
\item[$Duct$] is a family of functions, each is a convolution $F \otimes E$ of the field response functions $F$ associated with the sense wire and the element of drifting charge and the electronics response function $E$ corresponding to the shaping and amplification of the \dword{fe} electronics. More details can be found in Section~\ref{sec:tools-mc-daq}.
\item[$Digit$] models the digitization electronics according to a given sampling rate, resolution, and dynamic voltage range and baseline offset resulting in an \dword{adc} waveform. 
\item[$Noise$] simulates the inherent electronics noise by producing a voltage level waveform from a random sampling of a Rayleigh distributed amplitude spectrum and uniformly distributed phase spectrum.  The noise spectra used are from measurements with the \dword{pdsp} detector after software noise filters, which have excess (non-inherent) noise effects removed.
\end{description}
%
These functions are defined over broad ranges and with fine-grained resolution. The resolutions are set by the variability (sub millimeter) and extent (several centimeters) of the field response functions and the sampling rate of the digitizer (\SI{0.5}{\micro\second}). Their ranges are set by the size of the detector (several meters) and the length of time over which one exposure is digitized (several milliseconds).  


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Data Acquisition Simulations and Assumptions}
\label{sec:tools-mc-daq}

%\begin{itemize}
%\item TPC electronics and noise simulation (Xin et al.) 
%\begin{itemize} \item Signal simulation/processing figure \end{itemize}
%\item Photon detector electronics simulation
%\end{itemize}

  The electrons ($\sim$5300 electrons per mm for MIP signals) on each wire are converted into raw wire signal (\dword{adc} vs Time) by convolution with the field response and electronics response, which is implemented in the Wire-Cell Toolkit software package~\cite{ref:wire_cell_toolkit}.
  The \dword{asic} electronics response was simulated with the BNL SPICE~\cite{spice} simulation.  For most samples, the \dword{asic} gain was set to \SI{14}{mV/fC} and the shaping time was set to \SI{2}{\micro\second}. There are several considerations in choosing the \SI{2}{\micro\second} shaping time setting (out of 0.5, 1.0, 2.0, 3.0 \SI{}{\micro\second}):
  \begin{itemize}
\item Since the digitization frequency is at 2 MHz, an anti-aliasing filter to ensure the satisfaction of the Nyquist theorem is required. This essentially excludes the \SI{0.5}{\micro\second} shaping time, which is not enough  to ensure complete anti-aliasing. 
\item A smaller shaping time in principle leads to a slightly better two-peak separation. However, since the drifting time of ionization electrons through one wire plane is about \SI{3}{\micro\second}, the difference between \SI{1}{\micro\second} and \SI{2}{\micro\second} shaping time is limited. 
\item The electronics noise, as parameterized by the standard deviation of the ADC values on each sample, is slightly lower for the \SI{2}{\micro\second} and \SI{3}{\micro\second} shaping-time settings than that of the \SI{1}{\micro\second} (by about 10\% or so). 
\end{itemize}
Te digitization is performed by a 12-bit ADC, which covers a range of about \SI{1.6}{V}. The number of bits is chosen so that the intrinsic noise
introduced by the digitization is negligible.  The instrinsic noise level was set to around 2.5 \dword{adc} RMS, based on extrapolation 
  from the MicroBooNE experiment~\cite{Acciarri:2017sde}. This value was further validated in \dword{pdsp}. % experiment.
  Figure~\ref{elec_resp} shows the expected electronics shaping functions.

\begin{dunefigure}
[ASIC's electronics shaping functions]
{elec_resp}
{The shaping functions of the Front-End ASIC, shown for the four shaping
time settings at \SI{14}{mV/fC} gain.}
\includegraphics[width=0.7\textwidth]{elec.png}
\end{dunefigure}



The \dword{pd} electronics simulation separately generates waveform 
for each channel (\dword{sipm}) of a \dword{pd} that has been hit by photons.
Every detected photon %that has been detected 
appears as a single \phel{} pulse
(with the shape taken from~\cite{http://lss.fnal.gov/archive/2015/pub/fermilab-pub-15-488-nd-ppd.pdf:2015gov}) on a randomly selected channel
(belonging to the \dword{pd} in which the photon was registered).
Then dark noise (with the rate of \SI{10}{Hz}) and 
line noise (Gaussian noise with the RMS of $2.6$ \dword{adc} counts) are added.
Each photon (or a dark-noise pulse) has a probability of appearing
as $2$ \phel{}s on a waveform (the cross-talk probability is $16.5~\%$).
The final step of the digitization process is recording only fragments
of the full simulated waveform that have a signal in them.
This is accomplished by passing the waveform through a hit finder
described in Sec.~\ref{sec:gaushit} 
and storing parts of the waveform corresponding to the hits found.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Event Reconstruction in the \dword{fd}}
\label{sec:tools-fdreco}

This section describes various reconstruction algorithms used to reconstruct events in the \dword{fd} TPC. A successful \lartpc reconstruction needs to deliver reconstructed tracks and showers, particle and event identification, particle momentum and event energy. The reconstruction starts with finding signals on each wire above a threshold and building ``hits'' out of each pulse. All the LArTPC \threed reconstruction algorithms share the same principle. The $x$ coordinate is determined by the drift time and the $y$ and $z$ coordinates are determined by the intersection of two wires on different planes with coincident hits. There are currently three different reconstruction approaches in the DUNE reconstruction package. The \twod$\rightarrow$\threed reconstruction approach starts with clustering together nearby hits on each plane, %together 
followed by the use of time information to match \twod clusters between different planes to form \threed tracks and showers. Examples of this approach include TrajCluster and \dword{pandora}.  The direct \threed approach reconstructs \threed points directly from hits and then proceeds to perform pattern recognition using those \threed points. Examples of this approach include SpacePointSolver and \dword{wirecell}. The third approach uses a deep-learning technique, known as a \dword{cnn}. There are several tools needed to complete the task of LArTPC reconstruction. These tools include track fitter (\dword{pma} or KalmanFilter), calorimetry, \dword{pid} and track momentum reconstruction using range for contained tracks or multiple Coulomb scattering for exiting tracks. In addition to the TPC reconstruction, the \dword{pd} reconstruction provides trigger and $t_0$ information for non-beam physics. 

%\begin{itemize}
%\item TPC reconstruction methods
%\begin{itemize}
%\item Demo figure for each method below
%\item TPC signal processing (Xin et al.)
%\item Hit reconstruction
%\item Track/Shower CNN
%\item TrajCluster
%\item PMA
%\item Pandora (Lorena)
%\item Wirecell (Xin et al.)
%\item SpacePointSolver
%\end{itemize}
%\item TPC Reco performance
%\begin{itemize} 
%\item Track finding efficiency vs. position, angle
%\item Shower purity/completeness comparison figures 
%\end{itemize}
%\item Photon detector reconstruction methods
%\begin{itemize}
%\item Hit finding
%\item Flash finding
%\begin{itemize} \item SN, PDK photon `event displays' \end{itemize}
%\item Calorimetry
%\end{itemize}
%\item Photon Reco Performance
%\begin{itemize}
%\item Efficiency vs. energy/position figures
%\item Timing resolution figure
%\item Energy resolution figure
%\end{itemize}
%\end{itemize}

\subsection{TPC Signal Processing}\label{sec:tpc_sp}

The raw data are in the format of \dword{adc} counts as a function of TPC ticks (\SI{0.5}{\micro\second} on each channel. The signal has a 
unipolar shape for a collection wire and a bipolar shape for an induction wire. The first step 
in the reconstruction is to reconstruct the distribution of ionization electrons arriving at the anode plane. 
This is achieved by passing the raw data through a deconvolution algorithm. In real detectors, excess 
noise may exist and require removal %need to be removed 
through a dedicated noise filter~\cite{Acciarri:2017sde}. 

The deconvolution technique was introduced to LArTPC signal processing in the context ArgoNeuT 
data analysis~\cite{Baller:2017ugz}. The goal of the deconvolution is to ``remove'' the impact of field and 
electronics responses from the measured signal along the time dimension in order to reconstruct the number of ionized
electrons. This technique has the advantages of being robust and fast, and is an essential step in the overall drifted-charge profiling process. This 1D deconvolution procedure was improved to a \twod deconvolution procedure 
by the MicroBooNE collaboration~\cite{Adams:2018dra,Adams:2018gbi}, which further took into account the long-range 
induction effects in the spatial dimension. Two-dimensional software filters (channel and time) are implemented to 
suppress high-frequency noise after the deconvolution procedure. For induction plane signals, regions of interest 
(ROIs) are selected to minimize the impact of electronics noise. More details of this algorithm can be found in~\cite{Adams:2018dra}.

This procedure, implemented in the Wire-Cell toolkit software package~\cite{ref:wire_cell_toolkit},  has been used in the TPC signal processing in \dword{pdsp}. Figure~\ref{pDUNE_sp_wf}
shows an example induction U-plane waveform before and after the signal processing procedure. The bipolar 
shape is converted into a unipolar shape after the \twod deconvolution. Figure~\ref{pDUNE_sp_example}
shows the full \twod image of induction U-plane signal from a \dword{pdsp} 
event~\cite{ref:pdune_signal_processing}. The measured signal (left) has a bipolar shape with red (blue) color 
representing positive (negative) signals. The deconvolved signal after 
the \twod deconvolution procedure (right) represents the reconstructed 
distribution of ionization electrons arriving at the anode wire plane. The 
deconvolved signal becomes unipolar, and the long-range 
induction effect embedded in the field response is largely removed. 

\begin{dunefigure}
[Measured and deconvolved waveform from an induction U-plane channel of ProtoDUNE-SP]
{pDUNE_sp_wf}
{An example of measured (black) and deconvolved waveform from an induction U-plane channel of \dword{pdsp}
before and after the signal processing procedure. For the measured waveform, the unit is \dword{adc}. For the deconvolved waveform, the unit is number of electrons after scaling down by a factor of 125.
Bipolar signal shapes are converted into unipolar signal shapes after \twod deconvolution.}
\includegraphics[width=0.95\textwidth]{sample_5141_23865.pdf}
\end{dunefigure}

\begin{dunefigure}
[Raw  and deconvolved induction U-plane signals from a ProtoDUNE-SP event]
{pDUNE_sp_example}
{Comparison of raw (left) and deconvolved induction U-plane signals (right) before and after 
the signal processing procedure from a \dword{pdsp} event. The bipolar shape with red (blue) color representing
positive (negative) signals is converted to the unipolar shape after the \twod deconvolution.}
\includegraphics[width=0.49\textwidth]{protodune_raw_u.pdf}
\includegraphics[width=0.49\textwidth]{protodune_decon_u.pdf}\end{dunefigure}


\subsection{Hit and Space-Point Identification}

\subsubsection{Gaussian Hit Finder}\label{sec:gaushit}

The reconstruction algorithms currently employed by \larsoft are based on finding hits on the deconvolved waveforms for each plane. A key assumption is that the process of deconvolution will primarily result in gaussian-shaped charge deposits on the waveforms and this drives the design of the gaussian hit finder module. Generally, the module loops over the input deconvolved waveforms and handles each in three main steps: first it searches the waveforms %are searched 
for candidate pulses, it then fits these candidates %are then fit 
to a gaussian shape and, finally, it places the resulting hit %is then placed 
in the output hit collection. Not all charge deposits will be strictly gaussian shaped, for example a track can emit a delta ray and it can take several wire spacings before the two charge depositions are %will be 
completely separated. Alternatively, a track can have a trajectory at large angles to the sense wire plane creating a charge deposition over a large number of waveform ticks. The candidate peak-finding stage of the hit finder attempts to resolve the individual hits in both of these cases, still under an assumption that the shape of each individual charge deposition is %will be 
gaussian. If this results in candidate peak trains that are ``too long'' then special handling  breaks these into a number of evenly-spaced hits and bypasses the hit-fitting stage. 

Figure~\ref{pDUNE_sp_hits} displays the results of the gaussian hit finder for the case of two or three hits only barely separated in \dword{pdsp} data. In this figure the deconvolved waveform is shown in blue, the red line represents the fit of the candidate peak to two or three gaussian shapes, the crosses represent the centers of the fit peaks, the pulse heights above the waveform baseline and their fit widths. 

\begin{dunefigure}
[An example of reconstructed hits in ProtoDUNE-SP data]
{pDUNE_sp_hits}
{An example of reconstructed hits in \dword{pdsp} data. The deconvolved waveform is shown in blue, the red line represents the fit of the candidate peak to two or three gaussian shapes, the crosses represent the centers of the fit peaks, the pulse heights above the waveform baseline and their fit widths.}
\includegraphics[width=\textwidth]{hits.png}
\end{dunefigure}


\subsubsection{Space Point Solver}

\begin{figure}
\includegraphics[width=.5\linewidth]{evd_zy_pre_2302}
\includegraphics[width=.5\linewidth]{evd_zy_noreg_2302}\\
\makebox[.5\linewidth][c]{(a) All coincidences}
\makebox[.5\linewidth][c]{(b) Without regularization}\\
\includegraphics[width=.5\linewidth]{evd_zy_2302}
\includegraphics[width=.5\linewidth]{evd_zy_true_2302}
\makebox[.5\linewidth][c]{(c) With regularization}
\makebox[.5\linewidth][c]{(d) True charge distribution}\\

\caption[Event displays of SpacePointSolver performance]{Performance of SpacePointSolver on a simulated \dword{fd} neutrino interaction. The first panel shows the position of all triplet coincidences in the $zy$ view (looking from the side of the detector), displaying multiple ambiguous regions. The second and third panels show the solution with and without regularization, the regularization disfavouring various erroneous scattered hits. The final panel shows the true charge distribution, demonstrating %that 
the fidelity of the regularized reconstruction.}

\label{fig:spacepoint}
\end{figure} % Anne left as is May 29; should become a dunefigure

The SpacePointSolver algorithm aims to transform the three \twod views provided by the wire planes into a single collection of \threed ``space points.''

First, triplets of wires are found with hits that are coincident in time within a small window (corresponding to \SI{2}{mm} in the drift direction) and where the crossing positions of the wires are consistent within \SI{3.55}{mm}. In some cases a collection wire hit may have only a single candidate pair of induction hits and the space point can be formed immediately. Often though, there are multiple candidate triplets, for example when two tracks are overlapped as seen in one view.

SpacePointSolver resolves these ambiguities by distributing the charge from each collection wire hit between the candidate space points so as to minimize the deviations between the expected and observed charges of the induction wire hits

\begin{equation}
\chi^2 = \sum_i^{\rm wires} \left(q_i - \sum_j^{\rm points}T_{ij}p_j\right)^2
\end{equation}

where $q_i$ is the charge observed in the $i^{\rm th}$ induction hit, $p_j$ is the solved charge at space point $j$, and $T_{ij}\in\{0,1\}$ encodes whether space point $j$ is coincident in space and time with wire hit $i$.

The minimization is subject to the condition that each predicted charge $p_j\ge0$, and that the total predicted charge for each collection wire hit exactly matches observations:

\begin{equation}
\sum_j^{\rm points}U_{jk}p_j=Q_k
\end{equation}

where $Q_k$ is the charge observed on the $k^{\rm th}$ collection wire, and $U_{jk}$ encodes the coincides of space point charges with the collection wires.

The problem as formulated is convex and can thus be solved exactly in a deterministic fashion. A single extra term can be added to the expression while retaining this property:

\begin{equation}
\chi^2 \to \chi^2 - \sum_{ij}^{\rm points}V_{ij}p_ip_j .
\end{equation}

By setting $V_{ij}$ larger for neighboring points this term acts as a regularization such that solutions with a denser collection of space points are preferred. The $V$ function is chosen empirically to have an exponential fall-off with constant \SI{2}{cm}.
%The algorithm is described more completely in \cite{spacepoint}.

Figure \ref{fig:spacepoint} shows the performance of this algorithm on a sample \dword{fd} \dword{mc} event, demonstrating good performance at eliminating spurious coincidences, and the importance of the regularization term.

SpacePointSolver was developed with the intention of acting as the first stage of a fully \threed reconstruction for \dword{fd} neutrinos, but it has been successfully put to use in a more restricted role to solve the disambiguation problem in \dword{protodune}. The full problem is solved, but for this application the information retained is restricted to the drift volume to which the corresponding space points for each induction hit are assigned. This technique correctly resolves more than 99\% of hits while requiring less CPU time than the standard disambiguation algorithm.

The outcome of the SpacePointSolver reconstruction, which associates a \threed point with three hits on three wire planes, is used in the process of disambiguation for \dword{protodune} and has been tested and used as well for \dword{fd}. This process of disambiguation determines which wire segment corresponds to the energy deposited by the particle in the TPC, since the induction wires are wrapped in the \dword{fd} TPC design in order to save cost on electronics and minimise dead regions between \dword{apa}s, which as a consequence produces that multiple induction wire segments will be read out by the same electronic channel.


%\subsubsection{Disambiguation}
%The induction wires are wrapped in the \dword{fd} TPC design in order to save cost on electronics and minimize the sizes of the dead regions between \dword{apa}s. The consequence is that multiple induction wire segments will be read out by the same electronic channel. We need to determine which wire segment corresponds to the energy deposited by the particle in the TPC. This process is called disambiguation. 

%The \dword{fd} disambiguation algorithm was originally developed for the \dword{35t} geometry. It relies on the fact that the collection wires are not wrapped and the wire angles are slightly different for the two inductions views (44.3$^{\circ}$ versus 45.7$^{\circ}$) in the \dword{35t} so that any three wires from the three planes read out by the same three channels will never cross twice. The algorithm uses gaushit as input. All the collection hits are unambiguous. Each induction hit has one channel ID and several possible wire IDs corresponding to various wire segments read out by the same channel. We need to determine the correct wire ID % which wire ID is the correct one 
%for each induction hit. The disambiguation algorithm first loops over all collection hits. For each collection hit, it loops over all induction hits and looks for one hit on each of the two induction planes that are in time with the collection hit. Once the triplet of hits is found (one on each plane with a common time), the algorithm checks all possible wire IDs and looks for three wires that intersect. Once one and only one intersection is identified, the two induction wire IDs are assigned to the two induction hits and the ambiguation is resolved. Finally the algorithm loops over all unresolved induction hits. For each hit, it loops over all possible wire segments for that channel and chooses the wire segment that is closest to a resolved induction hit as the correct wire segment. 

%The \dword{protodune} disambiguation algorithm uses the outcome of the SpacePointSolver reconstruction, which associate a \threed point with three hits on three wire planes. The \threed point can be projected onto the induction plane and the ambiguity of the induction wire signal can be resolved by choosing the wire segment that is closest to the \threed point projection. 
%(RS: protoDUNE's APAs should see signals from single TPC only)

%\subsection{Blurred Cluster}\label{sec:BlurredCluster}
%The Blurred Cluster reconstruction method aims to construct two-dimensional shower-like clusters from deposits left in the detector by showers.  It specialises in shower reconstruction, especially in the separation of nearby showers in the reconstruction of, e.g., $\pi^0$ decay.  The algorithm first applies a weighted \twod Gaussian smearing to the hit map in order to introduce `fake hits' and distribute the charge deposited in the detector more realistically.  This proceeds by convolution of a Gaussian kernel, uniquely applied for each hit given information such as rough directionality of the showering particles and the width of the reconstructed hits in time in order to introduce the most accurate blurring possible.  Clustering follows by grouping neighbouring hits within the blurred region before removing any artificial hits and forming output clusters from the remaining hit collections.  BlurredCluster uses disambiguated gaushit as input and the output clusters are in turn used as input to the EMShower algorithm (see Section \ref{sec:EMShower}).  More details and detailed discussion are available in Ref~\cite{ref:blurredcluster}.

\subsection{Hit Clustering, Pattern Recognition and Particle Reconstruction}

There are different approaches for hit clustering, pattern recognition and particle reconstruction that are being explored in the context of DUNE \dword{fd} interactions. The main ones are described in this section.  

\subsubsection{Line Cluster}\label{sec:LineCluster}
The intent of the Line Cluster algorithm is to construct \twod line-like clusters using local information. The algorithm was originally known as Cluster Crawler. The ``Crawler'' name is derived from the similarity of this technique to ``gliders'' in \twod cellular automata. The concept is to construct a short line-like ``seed'' cluster of proximate hits in an area of low hit density where hit proximity is a good indication that the hits are indeed associated with each other. Additional nearby hits are attached to the leading edge of the cluster if they are similar to the hits already attached to it. The conditions are that the impact parameter between a prospective hit and the cluster projection is similar to those previously added and the hit charge is similar as well. These conditions are moderated to include high charge hits that are produced by large $dE/dx$ fluctuations and the rapid increase in $dE/dx$ at the end of stopping tracks while rejecting large charge hits from $\delta$-rays.
Seed clusters are formed at one end of the hit collection so that crawling in only one direction is sufficient. LineCluster uses disambiguated gaushits as input and produces a new set of refined hits. More details on the Line Cluster algorithm can be found in~\cite{ref:linecluster}.

\subsubsection{TrajCluster}\label{sec:TrajCluster}
TrajCluster reconstructs \twod trajectories in each plane. It incorporates elements of pattern recognition and Kalman Filter fitting. The concept is to construct a short ``seed'' trajectory of nearby hits. Additional nearby hits are attached to the leading edge of the trajectory if they are similar to the hits already attached to it. The similarity requirements use the impact parameter between the projected trajectory position and the prospective hit, the hit width and the hit charge. This process continues until a stopping condition is met such as lack of hits, an abnormally high or low charge environment, or encountering a \twod vertex or a Bragg peak.

\twod vertices are found between trajectories in each plane. The set of \twod vertices is matched between planes to create \threed vertices. A search is made of the ``incomplete'' \threed vertices, those that are only matched in two planes, to identify trajectories in the third plane that were poorly reconstructed.

Two recent additions to TrajCluster are matching trajectories in \threed and tagging of shower-like trajectories. More details on the TrajCluster algorithm can be found in~\cite{ref:trajcluster}.



\subsubsection{Pandora}\label{sec:Pandora}

The \dword{pandora} software development kit~\cite{Marshall:2015rfa} was created to address the problem of identifying energy deposits from individual particles in fine-granularity detectors, using a multi-algorithm approach to solving pattern-recognition problems. Complex and varied topologies in particle interactions, especially with the level of detail provided by \lartpc{}s, are unlikely to be solved successfully by a single clustering algorithm. Instead, the \dword{pandora} approach is to break the pattern recognition into a large number of decoupled algorithms, where each algorithm addresses a specific task or targets a particular topology. The overall event is then built up carefully using a chain of many tens of algorithms. The \dword{pandora} multi-algorithm approach has already been applied to \lartpc{} detectors, and has been successfully used in different analyses for the automated reconstruction of cosmic-ray muons and neutrino interactions in the MicroBooNE experiment~\cite{Acciarri:2017hat} as well as test beam interactions in the \dword{pdsp} detector (see Section~\ref{sec:Pandora:ProtoDUNE}).


%\subsubsection{Pandora Inputs and Outputs}

%The sole input to the Pandora pattern recognition is a list of reconstructed and disambiguated 2D Hits, along with detector information (such as dimensions, unresponsive or dead material regions, etc.). Via the {\it larpandora} repository, which integrates Pandora pattern recognition algorithms into the LArSoft framework, these hits are translated into native Pandora 2D hits and separated into the different views and into ``drift volumes'', defined as the regions of the detector with a common drift readout. The specified chain of pattern recognition algorithms is then initialised and applied. 

%The results of the pattern recognition are output into the ART/LArSoft framework, again via {\it larpandora}. The output of the reconstruction is illustrated in Figure~\ref{larsoft_output}, being the main output a list of reconstructed 3D particles (termed ``PFParticles'', where ``PF'' stands for Particle Flow) for each event. A PFParticle corresponds to a distinct track or shower in the event, and has associated collections (Clusters) of 2D hits for each view, as well as an associated set of reconstructed 3D positions (SpacePoints) and a reconstructed Vertex position that defines its interaction point or first energy deposit. Navigation along PFParticle hierarchies is achieved using the PFParticle interface, which connects parent and daughter PFParticles, providing the Particle Flow describing the interaction. In the reconstruction of neutrino interactions, a neutrino PFParticle is also created and forms the top-level parent particle in each reconstructed neutrino hierarchy, with only a Vertex associated to it. Pandora also provides a second level of ART associations linking elements such as Clusters, Slices, SpacePoints, Tracks and Showers with their corresponding collection of Hits. 

%Pandora uses the concept of {\it slice} internally to represent and separate subsets of hits topological distinct, based on distance and pointing information, with the aim of avoiding merging together into one hierarchy particles produced by the interaction of a neutrino or test beam particle and those with cosmic ray origin. An ART/LArSoft product is now also available to identify PFParticles and Hits that belong to the same {\it slice}. For PFParticles, this information is also stored in the Metadata object, which contains as well information handled internally by Pandora that might be helpful for usage downstream, namely whether a particle was identified as a clear cosmic, or the scores of the multivariate analysis tools, such as Boosted Decision Trees (BDTs) or Support Vector Machines (SVMs) that were used to identify the test team particle or the neutrino interaction respectively. 

%The identity of each particle is currently not reconstructed by Pandora, but PFParticles are instead characterised as track-like or shower-like using the topological information available at this stage, according to the pattern of their hits. Then, for each PFParticle a higher level object will be created, either a Track or a Shower depending on its classification, containing extra information such as direction, opening angle, etc.  

%\begin{figure}[!h!tbp]
%\centering
%\includegraphics[width=0.8\textwidth]{./FIG_LArPandoraOutputNew.png}
%\caption{Illustration of the Pandora output in ART/LArSoft. Navigation along PFParticle hierarchies is achieved using the PFParticle interface, represented by dashed lines. Navigation from PFParticles to their associated objects is represented by solid arrows, as well as the second level of ART associations from these objects to the underlying Hits input to Pandora.}
%\label{larsoft_output}
%\end{figure}

%\subsubsection{Overview of the Pandora Pattern Recognition Algorithms}

The input to the \dword{pandora} pattern recognition is a list of reconstructed and disambiguated \twod hits, alongside detector information (such as dimensions, unresponsive or dead material regions). The specified chain of pattern-recognition algorithms is applied to these input hits (once translated into native \dword{pandora} \twod hits). The results of the pattern recognition are persisted in the \dword{art}/\dword{larsoft} framework, with the major output being a list of reconstructed \threed particles (termed \dword{pfparticle}s). A \dword{pfparticle} corresponds to a distinct track or shower in the event, and has associated objects such as collections of \twod hits for each view (Clusters), \threed positions (SpacePoints) and a reconstructed Vertex position that defines its interaction point or first energy deposit. Navigation along \dword{pfparticle} hierarchies is achieved using the \dword{pfparticle} interface, which connects parent and daughter \dword{pfparticle}s, providing a Particle Flow description of the interaction. The identity of each particle is currently not reconstructed by \dword{pandora}, but \dword{pfparticle}s are instead characterized as track-like or shower-like based on their topological features. 

The main stages of the \dword{pandora} pattern recognition chain are outlined below, and are illustrated in Figure~\ref{reco_steps}. Note that both the individual pattern recognition algorithms and the overall reconstruction strategy are under continual development and will evolve over time, with a current emphasis %being the 
on the inclusion of machine-learning approaches to drive decisions in some key algorithms. The current chain of pattern-recognition algorithms has largely been tuned for neutrino interactions from the \fnal Booster Neutrino Beam; however, the algorithms are designed to be generic and easily reusable, and they are in the process of being adapted for neutrino interactions in the energy regime of DUNE. A more detailed description of the algorithms can be found in~\cite{Acciarri:2017hat}. 


\begin{enumerate}
\item{\bf Input hits:} The input list of reconstructed and disambiguated \twod hits are translated into native \dword{pandora} \twod hits and separated into the different views and into ``drift volumes'', defined as the regions of the detector with a common drift readout.
\item{\bf \twod track-like clusters:}  The first phase of the \dword{pandora} pattern recognition is track-oriented \twod clustering, creating ``proto-clusters'' that represent continuous, unambiguous lines of \twod hits. This early clustering phase is careful to ensure that the proto-clusters have high purity (i.e., represent energy deposits from exactly one true particle) even if this means they are initially of low completeness (i.e., only contain a small fraction of the total hits within a single true particle). A series of cluster-merging and cluster-splitting algorithms then examine the \twod proto-clusters and try to extend them, making decisions based on topological information, aiming to improve completeness without compromising purity.
\item{\bf \threed vertex reconstruction:} The neutrino interaction vertex is an important feature point. Once identified, any \twod clusters can be split at the projected vertex position, reducing chances of merging particles in any view. Cluster-merging operations also take proximity to the vertex into account, in order to protect primary particles emerging from the vertex region, and ensure good reconstruction performance for interactions with many final-state particles. Pairs of \twod clusters from different views are first used to produce lists of possible \threed vertex positions. These candidate vertices are examined and scored, and the best vertex is selected. \dword{pandora} has developed different algorithms for the selection of the neutrino vertex, including the use of machine-learning approaches in MicroBooNE. Similar approaches can be harnessed in the future for interactions in %DUNE FD
the \dword{fd}, where a score-based approach is currently used.
\item{\bf \threed track reconstruction:} The aim of the \threed track reconstruction is to identify the combinations of \twod clusters (from the different views) that represent the same true, track-like particle. These \twod clusters are formally associated by the construction of a \threed track particle. During this process, \threed information can also be used to improve the quality of the \twod clustering. A \dword{pandora} algorithm considers all possible combinations of \twod clusters, one from each view, and builds (what is loosely termed) a rank-three tensor to store a comprehensive set of cluster-consistency information. This tensor can be queried to identify and understand any cluster-matching ambiguities. \threed track particles are first built for any unambiguous combinations of \twod clusters. Cases of cluster-matching ambiguities are then addressed, with iterative corrections to the \twod clustering being made to resolve the ambiguities and so enable \threed particle creation.
\item{\bf \twod and \threed shower reconstruction:} A series of topological metrics (additional use of some calorimetric information would be desirable in the future) are used to characterize each \twod cluster as track-like or shower-like. This information is analyzed to identify the longest shower-like clusters, which form the ``seeds'' or ``spines'' for \twod and \threed shower reconstruction. A recursive algorithm is used to add shower branches onto each top-level shower seed, then branches onto branches, etc. The \twod showers are then matched between views to form \threed showers, reusing ideas from the \threed track-matching procedure.
\item{\bf \twod and \threed particle refinement and event building:} Following the \threed track and shower reconstruction, a series of algorithms is used to improve the completeness of the reconstructed particles by merging together any nearby particles that are just fragments of the same true particle. Both \twod and \threed approaches are used, %with a typical approach being to 
where a typical approach uses combinations of \twod clusters (from different views) to identify features in \threed, or projects \threed features into each of the \twod views. This is a powerful demonstration of the \dword{pandora} rotational coordinate transformation system, which allows seamless use of \twod and \threed information to drive pattern-recognition decisions. Finally, \threed space points are created for each \twod input hit, and the \threed particle trajectories are used to organize the reconstructed particles into a hierarchy. Final-state particles can be navigated via parent-daughter links, thus reconstructing their subsequent interactions or decays. For neutrino interactions, a top-level reconstructed neutrino particle is created; it represents the primary particle in the hierarchy linking together the daughter final-state particles and provides the information about the neutrino interaction vertex.

\end{enumerate}

\begin{dunefigure}
[Main stages of the PANDORA pattern recognition chain]
{reco_steps}
{Illustration of the main stages of the \dword{pandora} pattern recognition chain: (1) Input Hits; (2) \twod track-like cluster creation and association; (3) \threed vertex reconstruction; (4) \threed track reconstruction; (5) Track/Shower separation; (6) \twod and \threed particle refinement and event building.}
\includegraphics[width=3.7cm, height=5.5cm]{./Pandora/OneInputHits.pdf}
\includegraphics[width=9.5cm, height=5.5cm]{./Pandora/TwoPrePostTopAssoc.pdf}
\includegraphics[width=3.7cm, height=5.5cm]{./Pandora/ThreeVertexCandidates.pdf}
\includegraphics[width=6.0cm, height=5.5cm]{./Pandora/Four3DMatch.pdf}
\includegraphics[width=4.0cm, height=5.5cm]{./Pandora/FiveTrackShower.pdf}
\includegraphics[width=8.8cm, height=5.5cm]{./Pandora/SixPrePostMopUp.pdf}
\end{dunefigure}


The algorithms forming the stages described above can be used in different ways, thanks to the multi-algorithm approach. Currently, two \dword{pandora} reconstruction paths ({\it Pandora Cosmic} and {\it Pandora Neutrino}) have been created, using chains of tens of algorithms each (note that over 130 algorithms and tools are used in total). Although many algorithms are shared between the two paths, the overall algorithm selection results in different key features:
\begin{enumerate}
\item Pandora Cosmic: Strongly track-oriented, optimized for the reconstruction of cosmic-ray muons and their daughter (shower-like) delta rays. 
\item Pandora Neutrino: Optimized for the reconstruction of neutrino or test beam particle interactions, carefully building the event using the reconstructed interaction vertex (protecting particles emerging from it) and including a careful treatment of tracks versus showers. 
\end{enumerate}

These two chains of algorithms are harnessed together to provide a consolidated output in the case of surface detectors exposed to cosmic rays, such as MicroBooNE and \dword{pdsp} (without significant cosmic-ray background, only the Pandora Neutrino algorithm chain is necessary for the \dword{fd}). The overall reconstruction strategy in such detectors is illustrated in Figure~\ref{consolidated_reco}. It starts by running the Pandora Cosmic reconstruction on the entire collection of input hits, then identifies ``clear'' cosmic rays. This identification uses a geometrical approach to tag through-going cosmic rays and examines the consistency of the cosmic rays with the $t_{0}$ appropriate to the neutrino beam spill. Clear cosmic rays are output at this stage. For the remaining ambiguous hits, however, additional stages are required. A \textit{slicing} process is applied to the remaining hits, dividing them into smaller regions (slices) that represent separate, distinct interactions. Each slice is reconstructed using both the Pandora Neutrino and Pandora Cosmic reconstruction chains and the results are compared directly to identify whether the slice corresponds to a cosmic ray or a neutrino interaction (in the case of MicroBooNE) or test beam interaction (in the case of \dword{pdsp}). The consolidated event output is formed of three classes of reconstructed particles: (1) clear cosmic rays, (2) cosmic rays that are spatially and temporally consistent with being a neutrino interaction in the detector (remaining cosmic-rays) and (3) candidate neutrino or test beam interactions.

\begin{dunefigure}
[Schema of PANDORA consolidated output and reconstruction strategy for surface LArTPCs]
{consolidated_reco}
{Schema of the \dword{pandora} consolidated output and overall reconstruction strategy for surface \dwords{lartpc} such as \dword{microboone} and \dword{pdsp}. See text for more details.}
\includegraphics[width=0.65\textwidth]{./Pandora/NewConsolidatedDiagram.png}
\end{dunefigure}



Of particular importance in this overall reconstruction strategy is the neutrino (MicroBooNE) or test beam particle (\dword{pdsp}) identification tool. This tool is responsible for deciding whether to output the cosmic ray or neutrino (or test beam) reconstruction outcomes for a given slice. For \dword{pdsp}, this decision is based on the output from adaptive \dwords{bdt}, trained to distinguish between cosmic-ray and test beam particles, which has proved to be highly efficient across the momentum range of \dword{pdsp} data (see Section ~\ref{sec:Pandora:ProtoDUNE}). %The variables used in this BDT model have proved to be highly efficient across the momentum range of ProtoDUNE-SP data (as it will be shown in Section ~\ref{sec:Pandora:ProtoDUNE}). These variables are: 

%\begin{itemize}
%\item The eigenvalues of the covariance matrix of the spatial position of the 3D LArTPC hits.
%\item The distance of the closest 3D hit to the beam spot.
%\item The vertical distance of the reconstructed 3D hit closest to the top of the detector.  
%\item The number of reconstructed particles.
%\item The angle and direction of a spatial fit to the reconstructed 3D hits with respect to the beam line.
%\end{itemize}

The performance obtained with the current algorithms are shown in Section~\ref{sec:performance}, both for the \dword{fd} and \dword{pdsp}.  As previously mentioned, both the individual pattern recognition algorithms and the overall reconstruction strategy are under continual development. Many algorithms %are still to be explicitly tuned 
still require explicit tuning for the DUNE energy ranges, and new algorithms, designed specifically for DUNE, will be added to the multi-algorithm pattern recognition. The performance presented in this document therefore represents a current snapshot and is expected to improve with future dedicated work. 


\subsubsection{Projection Matching Algorithm}\label{sec:PMA}
\dword{pma}  was primarily developed as a technique of \threed reconstruction of individual particle trajectories (trajectory fit) Ref~\cite{Antonello:2012hu}. \dword{pma} was designed to address a challenging issue of transformation from a set of independently reconstructed \twod projections of objects into a \threed representation. Reconstructed \threed objects are also providing  basic physics quantities like particle directions and $dE/dx$ evolution along the trajectories. \dword{pma} uses as its input the output from \twod pattern recognition: clusters of hits. For the purposes of the DUNE reconstruction chain the Line Cluster algorithm (Section~\ref{sec:LineCluster}) is used as input to \dword{pma}, however the use of hit clusters prepared with other algorithms may be configured as well. As a result of \twod pattern recognition, particles may be broken into several clusters of \twod projections, fractions of particles may be missing in individual projections, and clusters obtained from complementary projections % are not guaranteed to 
may not cover corresponding sections of trajectories. Such behavior is expected since ambiguous configurations of trajectories can be resolved only if the information from multiple \twod projections is used. Searching for the best matching combinations of clusters from all \twod projections was introduced to the \dword{pma} implementation in the \dword{larsoft} framework. The algorithm also attempts to correct hit-to-cluster assignments using properties of \threed reconstructed objects. In this sense \dword{pma} is also a pattern-recognition algorithm.
The underlying idea of \dword{pma} is to build and optimize objects in \threed space (formed as polygonal lines with % an iteratively increased number of segments) 
the number of segments iteratively increased) by minimizing the cost function calculated simultaneously in all available \twod projections. %The cost function consists of the \twod distance of hits to the optimized object \twod projections, penalty of tracks curvature, and \threed distance of various feature points to the optimized object (used e.g., to improve performance for tracks with isochronous orientation).
%\fixme{please clarify prev sentence. anne} 
%The track can be reconstructed using clusters from two projections while the distance of hits to the track projection in the third plane is used to validate correct association of clusters. This method is used to score \threed track candidates in the three-plane TPC configurations, like the \dword{spmod}, %single-phase DUNE 
%MicroBooNE, %detectors 
%and prototypes. In this scenario clusters from all planes are used in the fine-tuning of the selected candidates. %In the two-plane configurations (\dual TPC detectors, \single LArIAT and ArgoNeut  detectors) the track candidates are scored by the value of the cost function, which is significantly increased if the trajectory fit is being optimized to spuriously associated clusters
 %\fixme{if we optimize the fit to do what with spuriously associated clusters? anne}(this is also a second-level criterion for the three-plane configurations %if 
 %in cases where the validation method shows no significant difference for track candidates).
%The approach of constructing entire objects in \threed %allows to avoid requirement of 
%avoids the need to find associations between \twod planes on the level of individual hits. Such associations are especially problematic if several hits on a single trajectory can be found with a similar drift time value, or, if  due to a low signal, for example, % (or any other reason) 
%hits are missed in one of projections. Unambiguous \threed position is calculated for each \twod hit, independently from other hits in the trajectory. Such \threed positions are more accurate than found with the ``standard'' calculation of a single \threed point using multiple \twod hits matched by drift time values, leading to the more accurate $dE/dx$ estimation. It also allows %to build 
%construction of \threed objects using the detector data from \twod planes directly, without an intermediate step of %a \threed points calculation which are subsequently 
%calculating \threed points for subsequent use in obtaining the final trajectory fit. Another advantage of the approach is the capability to estimate direction of small, few-hit tracks, %or estimation of 
%and to estimate the \threed PC axis of shower-like objects. \fixme{what's PC? anne}
Several features were developed in \dword{larsoft}'s \dword{pma} implementation to address detector-specific issues like stitching the particle fragments found in different TPCs or %an option for 
performing disambiguation at the \threed reconstruction stage. Since algorithms existing within or interfaced to the \dword{larsoft} framework (see Section \ref{sec:Pandora}) can provide pattern reconstruction results that include the particle hierarchy description, the mode for applying \dword{pma} to calculate %solely the 
trajectory fits alone was developed. In this mode the collections of clusters forming particles are taken from the ``upstream'' algorithm and  hit-to-cluster associations remain unchanged. %are not changed.


%Recent developments of PMA are relying on the same principle ideas and allow to build and optimize complex structures of \threed objects, i.e. multiple particle trajectories interconnected with interaction vertices. With such approach it is possible to employ in the vertex position reconstruction the local information from several tracks simultaneously, leading also to an improved fit of each individual trajectory. The track-vertex structure is constructed after the individual trajectories are found and stitched across TPC's. Vertex candidates are found as regions of intersection of two or more tracks (including regions found beyond the track endpoints), with the threshold on the allowed region size. Tracks shorter than $nn$ cm are treated separately; they can be associated to vertex candidates found using long tracks in the first pass of the algorithm or used to create vertex candidates in the second pass. Candidates are scored by the number of intersecting tracks, and if this number is equal for two candidates the maximum angle between intersecting tracks is used select candidate and create a better defined vertex as first. The entire track-vertex structure with the newly created vertex is re-optimized using PMA principles in order to accommodate additional information in the trajectory fits. Since the new vertices may connect structures that already contain vertices, a set of rules for splitting and flipping tracks was developed to ensure that the resulting structure is always tree-like (i.e. does not contain loops). The resulting structure is described as a particle hierarchy using data products available in \dword{larsoft}.

%\fixme{orig of next pgraph is commented above; I started rewording but I can't parse enough of it unambiguously. anne}
%Recent \dword{pma} developments rely on the same principle and allow building and optimizing complex structures of \threed objects, e.g., multiple particle trajectories interconnected with interaction vertices. This makes it possible to use the local information from several tracks simultaneously in the vertex position reconstruction, which leads to an improved fit for each individual trajectory. 
%
%\dword{pma} constructs the track-vertex structure after it finds the individual trajectories and stitches them across TPCs. Vertex candidates are found as regions of intersection of two or more tracks (including regions found beyond the track endpoints), with the threshold on the allowed region size. 
%\fixme{what threshold? and fix nn cm below}
%Tracks shorter than $nn$ cm are treated separately; they can be associated to vertex candidates found using long tracks in the first pass of the algorithm or \fixme{}used to create vertex candidates in the second pass. Candidates are scored by the number of intersecting tracks, and if this number is equal for two candidates the maximum angle between intersecting tracks is used select candidate and create a better defined vertex as first. The entire track-vertex structure with the newly created vertex is re-optimized using \dword{pma} principles in order to accommodate additional information in the trajectory fits. Since the new vertices may connect structures that already contain vertices, a set of rules for splitting and flipping tracks was developed to ensure that the resulting structure is always tree-like (i.e., does not contain loops). The resulting structure is described as a particle hierarchy using data products available in \dword{larsoft}.


%Two important reconstruction features are currently under development in \dword{larsoft}'s \dword{pma} implementation. The first is the integration of \dword{pma} input with the recognition 
%\fixme{integration with the `recognition'? Does that mean to add recognition capability to PMA? anne} of shower-like \twod objects (e.g., electromagnetic showers, electron tracks) and track-like objects (e.g., hadron and muon tracks). Such recognition is a prerequisite of obtaining robust \threed tracking with \dword{pma}, since different fitting strategies should be applied to track and shower objects. The second %feature under development %is the detection of kinks and decay points missed at the \twod pattern recognition level. First attempts were made %with the search 
%made by searching for outliers in the distribution of angles in the trajectory fit. It was found that the dependence on the track orientation in a \twod projection needs to be taken into account. In addition, a potential integration with an algorithm based on \twod \dword{adc} image analysis will be explored.


%\subsection{EMShower}\label{sec:EMShower}

%mode is used when reconstructing neutrino events.The EMShower reconstruction algorithm aims to find final 3D showers and all associated properties.  It is intentionally high-level by design and relies heavily on previous reconstruction, specifically BlurredCluster (Section \ref{sec:BlurredCluster}) and PMA (Section \ref{sec:PMA}).  The reconstruction proceeds in two general steps: first, the shower objects, including all associated hits in each of the views, are found; second, the properties of these showers, such as start point, direction, energy and initial dE/dx, are determined by multiple pattern recognition and calorimetric reconstruction algorithms.

%The shower objects are created by simply matching the previously found, well-formed, shower-like clusters (provided by BlurredCluster) between the different views to form 3D objects with associated hits in each plane.  This is achieved by associating hits between the \twod shower-like clusters and 3D tracks (provided by, e.g., PMA) in order to pull together clusters from different planes into one object.  These shower hits are then analysed by various successive algorithms in order to find relevant properties before the output shower objects are constructed for later use.

%EMShower can be configured to use hits from the shower-like PFParticles identified by the Pandora package. This mode is used when reconstructing neutrino events.

%Further details and detailed discussion can be found in Ref~\cite{ref:emshower}.

\subsubsection{Wire Cell}

\begin{dunefigure}
[Overview of wire-cell reconstruction paradigm]
{wire-cell-overview}
{Overview of the \dword{wirecell} reconstruction paradigm, taken from~\cite{ref:wc_talk}. 
See text for more details.}
\includegraphics[width=0.98\textwidth]{wire-cell-overview.png}\end{dunefigure}



The \Dword{wirecell} is a new reconstruction package under development. The current 
status of this reconstruction paradigm is shown in Figure~\ref{wire-cell-overview}. The 
simulation of the induction signal in a LArTPC and the overall signal processing process,
which are general to all reconstruction methods, are described in Sections~\ref{sec:tpc_sim} 
and~\ref{sec:tpc_sp}, respectively. The subsequent reconstruction in \dword{wirecell} adopts 
a different approach from the aforementioned algorithms. Instead of directly performing pattern 
recognition on each of the \twod views (drift time versus wire number), \threed imaging of events 
is obtained with time, geometry, and charge information. This step is independent from 
the event topologies, and the usage of the charge information takes advantage of a unique 
feature of the projection views, as each of the wire plane detects the same 
amount of the ionization electrons under transparency condition. The strong requirement of the time, geometry, and charge 
information provides a natural way to suppress electronic noise
 while combining with successful signal processing maintains high hit efficiency. Details of this step is described in~\cite{Qian:2018qbv}. The subsequent reconstruction involves the object clustering and
TPC and light matching, which has been crucial for selecting neutrino interactions in the 
MicroBooNE experiment~\cite{uboone_wc_note}. The current focus of the \dword{wirecell} algorithm 
development is on the trajectory and $dQ/dx$ fitting, which aims at enabling precision particle
identification in a \lartpc. %Beside these, 
Development of \threed pattern recognition also needs
to be revisited before reaching a complete reconstruction chain. 

\begin{dunefigure}
[\threed display of interaction in ProtoDUNE-SP]
{wire-cell-bee}
{This \threed display shows the full size of the \dword{pdsp} detector (gray box) and 
the direction of the particle beam (yellow arrow). Particles from other sources (such as cosmic rays) 
can be seen throughout the white box, while the red box highlights the region of interest: 
in this case, an interaction resulting from the 7 GeV beam particle through the detector. 
The \threed points are obtained using the Space~Point~Solver reconstruction algorithm. This event
can be accessed through interactive web-based event display Bee at \url{https://www.phy.bnl.gov/twister/bee/set/protodune-gallery/event/0/}.}
\includegraphics[width=0.85\textwidth]{bee_event.png}
\end{dunefigure}



The \dword{wirecell} team also created an advanced web-based \threed event display, ``Bee''~\cite{wire-cell-bee}, to aid the reconstruction development and provide interactive visualizations to end users.  Bee, together with \twod Magnify event 
display tools, have played important roles in 
the development of various reconstruction algorithms, including signal processing, \threed event 
imaging, object clustering, TPC and light matching, and trajectory and $dQ/dx$ fitting. The Bee event display 
was also used during the ProtoDUNE data-taking period to stream real-time reconstructed events to the users.
Figure~\ref{wire-cell-bee} shows an example of a data event from the \dword{pdsp} detector~\cite{ref:wc_bee}. 
The full video of this event can be found in~\cite{ref:bee_video}.

%\fixme{The paragraph below is too much technical detail for a physics TDR.  I propose to comment it out.  -- JU}

%The primary software package hosting various \dword{wirecell} reconstruction algorithm is the 
%\dword{wirecell} toolkit (WCT)~\cite{ref:wire_cell_toolkit}, 
%which provides a consistent, 
%ull-featured configuration system based on the Jsonnet \cite{jsonnet} data templating 
%language and its C++ bindings.  Individual component construction and the application of
%configuration data are handled by the toolkit, which allows for dynamic
%construction patterns.  WCT supports a number of component execution models and in particular
%provides one powerful built-in  model that follows the data flow
%programming paradigm.  It connects toolkit components as nodes in a
%directed graph that exchange data along the graph edges.  A flow graph
%may be constructed in C++ or more simply via user configuration. The execution of a WCT 
%flow graph is itself an abstracted component.  Two implementations currently exist.
%One is single-threaded and minimizes memory usage.  The
%second, based on Intel TBB~\cite{tbb} allows for multi-threaded
%concurrent execution of components and exchanges data across graph edges
%%which are 
%implemented as thread-safe queues. The development of WCT follows a toolkit approach
%and is designed to be easily integrated into larger applications. The toolkit makes strong use of 
%abstractions for all public data objects and code units. With a shared library 
%plugin system, the abstract interface implementation is made available to the application
%using the toolkit. Currently, WCT has been integrated into the \dword{art} framework via a
%package in the \dword{larsoft} software suite.  It provides an \dword{art} tool as the primary 
%interface to the WCT as well as a number of WCT components which convert between \dword{larsoft} 
%and WCT data representations. This interface is in regular use to run both MicroBooNE and 
%\dword{pdsp} WCT jobs.



\subsubsection{Deep Learning}\label{sec:deeplearning}

Deep learning methods are used %There are 
in two main areas of the DUNE event reconstruction. % where . 
Both of these algorithms are based on \dwords{cnn}. 
In recent years \dwords{cnn} have become the method of choice for many image recognition tasks in commerce and industry, and lately have been applied to high energy physics. The \dwords{cnn} contain a series of filters that are applied to the input detector data images in order to extract the features required to classify the images.

\subsubsection{\dword{cnn} for track and shower separation}
The hit-level \dword{cnn} aims to classify each reconstructed hit as either track-like or shower-like by looking at the local region surrounding the hit in (charge, time) coordinates.  The \dword{cnn} is trained using a large number of simulated images with the known true origin of the energy deposits. Once trained, the \dword{cnn} provides the track-like or shower-like classification for each hit object in the event. This algorithm is applied to each readout view in each TPC separately. %The output from the algorithm is shown in Figure \ref{fig:trkshw_CNN_protodune} for a beam interaction in ProtoDUNE-SP. 

\subsubsection{\dword{cnn} for event selection}
The algorithm used for the classification of neutrino interaction types is called the \dword{cvn} and is  based on a \dword{cnn}. The primary goal of the \dword{cvn} is to provide a probability for each neutrino interaction to be \dword{cc}$\,\nu_\mu$, \dword{cc}$\,\nu_e$, \dword{cc}$\,\nu_\tau$ or \dword{nc}. The \dword{cvn} takes three $500\,\times\,500$ pixel images of the neutrino interactions as input, one from each view. The images contain the charge and the peak time of the reconstructed hits and does not use any information beyond the hit reconstruction. The \dword{cvn} is discussed in more detail in the \dword{lbl} chapter of the \dword{tdr} Physics Volume. 


\subsection{Calorimetric Energy Reconstruction and Particle Identification}

As charged particles traverse a \lar{} volume, they deposit energy through ionization and scintillation. It is important to measure the energy deposition, as it provides information on particle energy and species. The algorithm for reconstructing the ionization energy in \dword{larsoft} is optimized for line-like tracks and is being extended to more complicated event topology such as showers. The algorithm takes all the hits associated with a reconstructed track and for %. For 
each hit, it converts the hit area or amplitude, in \dword{adc} counts, %is converted 
to the charge $Q_{\rm det}$, in units of \si{\femto\coulomb}, on the wire using an \dword{adc}-to-\si{\femto\coulomb} conversion factor that was determined by muons or test stand measurements. To account for the charge loss along the drift due to impurities, a first correction is applied to $Q_{\rm det}$ to get the free charge after recombination $Q_{\rm free} = Q_{det}/e^{-t/\tau_{e}}$, where $t$ is the electron drift time for the hit and $\tau_{e}$ is the electron lifetime measured by the muons or purity monitors. The charge $Q_{\rm{free}}$ is divided by the track pitch $dx$, which is defined as wire spacing divided by the cosine of the angle between the track direction and the direction normal to the wire direction in the wire plane, to get the $dQ_{\rm{free}}/dx$ for the hit. Finally, to account for charge loss due to recombination, also known as ``charge quenching,'' a second correction is applied to convert $dQ_{\rm{free}}/dx$ to $dE/dx$ based on the modified Box's model~ \cite{Acciarri:2013met} or the Birks's model~\cite{Amoruso:2004dy}. The total energy deposition from the track is obtained by summing the $dE/dx$ from each hit: $\sum\limits_{i}^{\rm all\ hits}(dE/dx)_{i}\cdot dx_{i}$.

If the incident particle stops in the LArTPC active volume, the energy loss $dE/dx$ as a function of the residual range ($R$), the path length to the endpoint of the track, is used as a powerful method for particle identification. There are two methods in \dword{larsoft} to determine particle species using calorimetric information. The first method calculates four $\chi^{2}$ values for each track by comparing measured $dE/dx$ %versus $R$ points to the proton, charged kaon, charged pion and muon hypotheses and identifies the track as the particle that gives the smallest $\chi^{2}$ value.
versus $R$ to hypotheses for the proton, charged kaon, charged pion and muon, and identifies the track as the particle that gives the smallest $\chi^{2}$ value. The second method calculates the quantity $PIDA = \langle A_{i}\rangle = \left\langle(dE/dx)_{i}R_{i}^{0.42}\right\rangle$ \cite{Acciarri:2013met}, which is defined to be the average of $A_{i} = (dE/dx)_{i}R_{i}^{0.42}$ over all track points where the residual range $R_{i}$ is less than \SI{30}{cm}. The particle species can be determined by making a selection on the $PIDA$ value. 

%Figure~\ref{dedx} shows the $dE/dx$ versus residual range and $PIDA$ distributions for reconstructed $K^{+}$, $\mu^{+}$ and $e^{+}$ tracks in proton decay events. The calorimetry information is very efficient in separating those three different particles.

%\begin{figure}[!ht]
%\subfloat[$K^{+}$]{\includegraphics[width=0.5\textwidth]{dedxktc.pdf}}
%\subfloat[$\mu^{+}$]{\includegraphics[width=0.5\textwidth]{dedxmutc.pdf}}\\
%\subfloat[$e^{+}$]{\includegraphics[width=0.5\textwidth]{dedxetc.pdf}}
%\subfloat[$PIDA$]{\includegraphics[width=0.5\textwidth]{pidakmue.pdf}}
%\caption{(a)(b)(c): $dE/dx$ as a function of the residual range for reconstructed $K^{+}$, $\mu^{+}$ and $e^{+}$ tracks in proton decay events ($p\rightarrow\bar{\nu}K^{+}$, $K^{+}\rightarrow\mu^{+}\rightarrow e^{+}$). The black curves are theoretical predictions. (d): $PIDA$ distributions of the reconstructed $K^{+}$, $\mu^{+}$ and $e^{+}$ tracks.}
%\label{dedx}
%\end{figure}


\subsection{Optical Reconstruction}

\subsubsection{Optical Hit Finder}
\label{sec:OpticalHitFinder}
The first step of the DUNE optical reconstruction is reading
individual waveforms from the simulated \dword{pd} electronics
and finding optical hits -- regions of the waveforms containing pulses.
The optical hit contains the optical channel (\dword{sipm}) that the hit
was found on, time corresponding to the hit, its width,
area, amplitude, and number of \phel{}s.


The current DUNE optical-hit-finder algorithm then searches for regions of the waveform
exceeding a certain threshold ($13$ \dword{adc} counts), checking whether that region
is wider than $10$ optical time ticks\footnote{The current simulation assumes a 
\SI{150}{MHz} digitizer like that used in protoDUNE, though the final far detector electronics
will use an \SI{80}{MHz} digitizer.}, and, if it is, calculating the aforementioned
optical-hit parameters for the region (including parts of the waveform around it
that have \dword{adc} values greater than $1$) and recording it as an optical hit.
The number of \phel{}s is calculated by dividing the full area of the hit
by the area of a single-\phel{} pulse.
The pedestal is assumed to be constant and is specified in the hit finder as $1500$ \dword{adc} counts (always correct for the MC).


\subsubsection{Optical Flash Finder}
After optical hits are reconstructed, they are grouped into higher-level objects called optical flashes.
The optical flash contains the time and time width of the flash,
its approximate $y$ and $z$ coordinates (and spatial widths along those axes),
its location and size in the wire planes,
the distribution of \phel{}s across all \dwords{pd},
and the total number of \phel{}s in the flash, among other parameters.

The flash-finding algorithm searches for an increase in \dword{pd} activity
(the number of \phel{}s) in time using information from optical hits
on all photon detectors.
When a collection of hits with the total number of \phel{}s  
greater than or equal to $2$ is found, the algorithm begins creating an optical flash.
It starts with the largest hit and adds hits from the found hit collection 
that lie closer than half the combined widths of the flash under construction
a nd the hit being added to it.
The flash is stored after no more hits can be added to it
and if it has more than two \phel{}s.

The algorithm also estimates spatial parameters of the optical flash
by calculating the number-of-photoelectron-weighted mean and 
root mean square of locations of the optical hits
(defined as centers of \dwords{pd} where those hits were detected)
contained in the flash.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Reconstruction Performance}
\label{sec:performance}

An automated reconstruction of the neutrino interaction events in DUNE, often complex topologies with multiple final state particles, is a significant challenge. The current chain of \dword{pandora} pattern recognition algorithms %used 
has been tuned for neutrino interactions from the \fnal Booster Neutrino Beam, and is in the process of being adapted for the wide range of energies of the DUNE \dword{fd}. Despite this, and thanks to the reusability of \dword{pandora} algorithms for different single phase \lartpc detectors,  good performance is already achieved with this first-pass pattern recognition, and output from Pandora is used in the computation of the energy reconstruction in the oscillation analysis. Significant improvements are expected  in the upcoming years with a more dedicated tune of the current algorithms, and the development of new ones, as needed. %according to the necessities of DUNE. 

%The following pages show figures assessing the performance of the current reconstruction. They are the result of the full reconstruction chain on the Monte Carlo simulations described in Section~\ref{sec:tools-mc}: i.e., signal processing, hit finder, disambiguation and Pandora pattern recognition. 
The current reconstruction performance, evaluated using metrics introduced in \ref{sec:Pandora:assessment}, is presented for simulated neutrino interactions in a \single \nominalmodsize \dword{fd} module in \ref{sec:Pandora:DUNEFD}, and for simulated and real data test beam events in \dword{pdsp} in \ref{sec:Pandora:ProtoDUNE}. These results outline the baseline performance on which improvements will continue to be made in the next years. In addition, examples of current high-level reconstruction performance are presented in~\ref{sec:Pandora:High}.

\subsection{Pandora Performance Assessment}
\label{sec:Pandora:assessment}
The performance of the \dword{pandora} pattern recognition is assessed by matching reconstructed \dword{pfparticle}s to the simulated \dword{mcparticle}s. These matches are used to evaluate the efficiency with which \dword{mcparticle}s are reconstructed as \dword{pfparticle}s, and to calculate the completeness and purity of each reconstructed \dword{pfparticle}. 

%\fixme{mc and pfparticles in gloss?}

The following procedure is used to match reconstructed \dword{pfparticle}s with simulated \dword{mcparticle}s:

\begin{itemize}
\item \textit{Selection of \dword{mcparticle}s:} The full hierarchy of true particles is extracted from the simulated neutrino interaction. A list of ``target'' particles is then compiled by navigating through this hierarchy and selecting the final-state ``visible'' particles producing a minimum number of reconstructed hits (allowed to be: $e^{\pm}$, $\mu^{\pm}$, $\gamma$, $\pi^{\pm}$, $\kappa^{\pm}$, $p$)\footnote{A minimum number of 15 reconstructed hits, with at least two views with 5 or more hits, is required in the definition of ``target'' \dword{mcparticle}. This  corresponds to true momentum thresholds of approximately 60 MeV for muons and 250 MeV for protons in the MicroBooNE simulation~\cite{Acciarri:2017hat}. Note that this selection is purely for performance assessment purposes, and that particles with fewer hits might still be created by \dword{pandora}.}. Any downstream daughter particles are folded in these target particles.
\item \textit{Matching of reconstructed \twod hits to \dword{mcparticle}s:} Each reconstructed \twod hit is matched to the target \dword{mcparticle} responsible for depositing the most energy within the region of space covered by the hit. The collection of \twod hits matched to each target \dword{mcparticle} is known as its ``true hits''.
\item \textit{Matching of \dword{mcparticle}s to reconstructed \dword{pfparticle}s:} The reconstructed \dword{pfparticle}s are matched to target \dword{mcparticle}s by analyzing their shared \twod hits. A \dword{pfparticle} and \dword{mcparticle} will be matched if the \dword{mcparticle} contributes the most hits to the \dword{pfparticle}, and if the \dword{pfparticle} contains the the largest collection of hits from the \dword{mcparticle}. The matching procedure is iterative, such that once each set of matched particles has been identified, these \dword{pfparticle}s and \dword{mcparticle}s are removed from consideration when making the next set of matches. 
\end{itemize}

Using the output of this matching scheme, the following performance metrics can be calculated:

\begin{itemize}
\item \textit{Efficiency:} Fraction of \dword{mcparticle}s with a matched \dword{pfparticle},
\item \textit{Completeness:} The fraction of \twod hits in a \dword{mcparticle} that are shared with its matched reconstructed \dword{pfparticle}, and 
\item \textit{Purity:} The fraction of \twod hits in a \dword{pfparticle} that are shared with its matched \dword{mcparticle}.
\end{itemize}

\subsection{Reconstruction Performance in the DUNE \dword{fd}}
\label{sec:Pandora:DUNEFD}

The performance of the \dword{pandora} pattern recognition has been evaluated using a sample of accelerator neutrino and antineutrino interactions simulated using the reference DUNE neutrino energy spectrum and the \nominalmodsize \dword{detmodule} geometry. The breakdown of the different interaction channels as a function of the true neutrino energy in the samples used is presented in Fig. \ref{breakdown_nuenergy}, for the events in the neutrino mode in which at least one ``target'' reconstructable \dword{mcparticle} is created and therefore evaluated. The following plots show that a good efficiency has already been achieved, and indicate particular regions and channels in which improvements can be made. 

\begin{dunefigure}
[Interaction channels vs true $\nu$ energy; simulated events used to assess performance]
{breakdown_nuenergy}
{Breakdown of the different interaction channels as a function of the true neutrino energy in the samples used in the assessment of reconstruction performance, for the simulated events in the neutrino mode in which at least one ``target'' reconstructable \dword{mcparticle} particle is created and therefore evaluated. Percentages indicate the fraction of each channel in the total number of events.}
\includegraphics[width=0.8\textwidth]{./Pandora/MCC11_NuE.png}
\end{dunefigure}

\begin{dunefigure}
[Reconstruction efficiency of PANDORA pattern recognition for a range of final-state particles at the FD]
{pandora_particle_efficiency}
{The reconstruction efficiency of the \dword{pandora} pattern recognition obtained for a range of final-state particles produced in all types of accelerator neutrino interactions except deep inelastic ones at DUNE FD. The efficiency is plotted as a function of the total number of \twod hits associated with the final-state \dwords{mcparticle} (summed across all views) on the top row, and as a function of the true momentum of the particle on the bottom row. Plots are shown for track-like particles (left) and shower-like particles (right) of each type leading in the event.}
\includegraphics[width=0.49\textwidth]{./Pandora/MCC11_ALL_BUT_DIS_HitsEff_Tracks_new.png}
\includegraphics[width=0.49\textwidth]{/Pandora/MCC11_ALL_BUT_DIS_HitsEff_Showers_new.png}
\includegraphics[width=0.49\textwidth]{./Pandora/MCC11_ALL_BUT_DIS_MomentumEff_Tracks_new.png}
\includegraphics[width=0.49\textwidth]{./Pandora/MCC11_ALL_BUT_DIS_MomentumEff_Showers_new.png}
\end{dunefigure}


Figure~\ref{pandora_particle_efficiency} shows the reconstruction efficiency as a function of the number of total true \twod hits and as a function of the true momentum for a range of final-state particles. The typical reconstruction efficiencies obtained for track-like \dword{mcparticle}s ($\mu^{\pm}$, $\pi^{\pm}$, $p$) rise from \SIrange{65}{85}{\%} for simulated particles depositing 100\,hits to \SIrange{85}{100}{\%} for particles with 1000\,hits. It should be emphasized that inefficiencies almost always result from accidental merging of multiple nearby true particles, rather than an inability to cluster hits from a true particle. The reconstruction efficiency for shower-like \dword{mcparticle}s ($e^{-}$,$\gamma$) is a bit lower than the equivalent for track-like particles at lower number of hits, but comparable with $>$100 hits.

Figure~\ref{pandora_completeness_purity} shows distributions of completenesses and purities for a range of final-state particles. In the case of final-state track-like particles, good completeness and purity are %both 
achieved, indicating that the track-based pattern recognition algorithms currently provide a high-quality reconstruction. It can be seen that final-state shower-like particles are typically reconstructed with high purity, but somewhat lower completeness, indicating that, although the shower reconstruction is fairly good already, there is room for addition of new algorithms specifically targeting an increase in shower completeness at DUNE.

For deep inelastic interactions, in which tens of final-state particles may be produced, a breakdown such as in Figures~\ref{pandora_particle_efficiency} and~\ref{pandora_completeness_purity} is less representative and informative (however, no significant impact has been observed when adding DIS events in the calculation of such quantities). Instead, Figure~\ref{pandora_dis} presents an assessment of the reconstruction of such events %is presented 
by comparing the number of reconstructed particles as a function of the number of true final-state particles in the event for \dword{nc} (left) and \dword{cc} (middle) deep inelastic interactions. These distributions are more populated in the diagonal, as they should be for perfect 1:1 reconstruction, indicating a good level of reconstruction of such events up to >5 final-state particles. In addition, the number of reconstructed particles matching the leading lepton in \dword{cc} deep inelastic interactions is also presented (right), which shows a consistently predominant single match for the leading lepton. 

Figure~\ref{pandora_vertex_resolution} shows distributions of the displacements $\Delta x$, $\Delta y$, $\Delta z$ and $\Delta R^{2} = (\Delta x)^2 + (\Delta y)^2 + (\Delta z)^2$ between the reconstructed and simulated neutrino interaction positions for all types of accelerator neutrino events. It can be seen that, for the vast majority of events, the reconstructed neutrino interaction vertex lies within $2$\,cm of the \dword{mc} truth in $x$, $y$ and $z$. While the $\Delta x$ and $\Delta y$ distributions are both symmetrical and sharply peaked around the origin, a small forward bias can be seen in the $\Delta z$ distribution. The reason for this bias comes from the fact that the neutrino interaction will be boosted in the forward $z$ direction, so vertex candidates are more likely created at $\Delta z>0$ than $\Delta z\,<\,0$.  

\begin{dunefigure}
[Completeness and purities for a range of final-state track-like  and shower-like particles]
{pandora_completeness_purity}
{Distributions of completenesses (top) and purities (bottom) for a range of final-state particles divided into track-like (left) and shower-like (right), produced in all types of accelerator neutrino interactions except deep inelastic ones at DUNE FD.}
\includegraphics[width=0.49\textwidth]{./Pandora/MCC11_ALL_BUT_DIS_Completeness_Tracks_new.png}
\includegraphics[width=0.49\textwidth]{./Pandora/MCC11_ALL_BUT_DIS_Completeness_Showers_new.png}
\includegraphics[width=0.49\textwidth]{./Pandora/MCC11_ALL_BUT_DIS_Purity_Tracks_new.png}
\includegraphics[width=0.49\textwidth]{./Pandora/MCC11_ALL_BUT_DIS_Purity_Showers_new.png}\end{dunefigure}

\begin{dunefigure}
[Number of reconstructed particles vs number of true final-state particles, CC and NC DIS events]
{pandora_dis}
{Distributions of number of reconstructed particles as a function of number of true final-state particles in deep inelastic events for neutral-current (left) and charged-current (right) %(middle)
interactions. In addition, the number of reconstructed particles matching the leading lepton in charged-current deep inelastic interactions is also presented (bottom).} %(right).}
%\includegraphics[width=0.315\textwidth]{./Pandora/MCC11_NCDIS.png}
%\includegraphics[width=0.315\textwidth]{./Pandora/MCC11_CCDIS.png}
%\includegraphics[width=0.315\textwidth]{./Pandora/MCC11_CCDISLeptonMatches.png}

\includegraphics[width=0.49\textwidth]{./Pandora/MCC11_NCDIS_new.png}
\includegraphics[width=0.49\textwidth]{./Pandora/MCC11_CCDIS_new.png}
\includegraphics[width=0.49\textwidth]{./Pandora/MCC11_CCDISLeadingLeptonMatches_new.png}
\end{dunefigure}

\begin{dunefigure}
[Displacements between reconstructed and simulated $\nu$ interaction vertices]
{pandora_vertex_resolution}
{The displacements between the reconstructed and simulated neutrino interaction vertices. The distributions are plotted for $x$ (top left), $y$ (top right), $z$ (bottom left) and $R^2$ (bottom right) and include all types of accelerator neutrino interaction (also deep inelastic events).}
\includegraphics[width=0.49\textwidth]{./Pandora/MCC11_ALL_INTERACTIONS_VtxDeltaX_new.png}
\includegraphics[width=0.49\textwidth]{./Pandora/MCC11_ALL_INTERACTIONS_VtxDeltaY_new.png}
\includegraphics[width=0.49\textwidth]{./Pandora/MCC11_ALL_INTERACTIONS_VtxDeltaZ_new.png}
\includegraphics[width=0.49\textwidth]{./Pandora/MCC11_VtxDeltaRsq.png}
\end{dunefigure}

\subsection{Reconstruction Performance in \dword{pdsp}}
\label{sec:Pandora:ProtoDUNE}

Further examination of the performance of the \dword{pandora} pattern recognition is provided through studies of the test-beam data taken by \dword{pdsp}.  Figure \ref{pandora_protodune_tbrecoeff} shows the reconstruction efficiency for triggered test-beam particles as a function of the momentum recorded by the trigger.  The reconstruction efficiency metric folds in many effects, including reconstruction, removal of cosmic-ray background and identification of the reconstructed particle as originating from the test beam.  An example of the \dword{pandora} reconstruction output for ProtoDUNE \dword{mc} simulations is shown in Figure~\ref{pandora_protodune_reco}.  For high-momenta test-beam particle interactions, a close agreement between the reconstruction efficiency for \dword{mc} simulations and data is observed in Figure~\ref{pandora_protodune_tbrecoeff}.  At high-momenta, the effect of beam-halo particles in the simulation appears to be overestimated, which results in the marginally lower reconstruction efficiency observed in simulation when comparison to data.  For low-momenta test-beam particle interactions, the reconstruction efficiency for data is significantly lower than that see in \dword{mc} simulations.  This is due to particles interacting between the trigger and the LArTPC before reaching its active volume in data.

\begin{dunefigure}
[Reconstruction efficiency for triggered test-beam particles as a function of particle momentum]
{pandora_protodune_tbrecoeff}
{The efficiency of reconstruction for the triggered test-beam particle as a function of particle momentum in data (red) and simulation (black).}
\includegraphics[width=0.75\textwidth]{./Pandora/ProtoDUNE/BeamParticleEfficiencyVsMomentum.pdf}
\end{dunefigure}




\begin{figure}[!ht]%Anne leaving as is 5/28
\centering
\subfloat[]{\label{fig:reco3d}\includegraphics[width=0.75\textwidth]{./Pandora/ProtoDUNE/TDR_EventDisplay3D.pdf}} \\ 
\subfloat[]{\label{fig:recou}\includegraphics[width=0.33\textwidth]{./Pandora/ProtoDUNE/TDR_EventDisplayU.pdf}}
\subfloat[]{\label{fig:recov}\includegraphics[width=0.33\textwidth]{./Pandora/ProtoDUNE/TDR_EventDisplayV.pdf}}
\subfloat[]{\label{fig:recow}\includegraphics[width=0.33\textwidth]{./Pandora/ProtoDUNE/TDR_EventDisplayW.pdf}}
\caption[Pandora reconstruction output for \SI{7}{GeV} MC test beam event]{An example of the \dword{pandora} reconstruction output for a 7 GeV Monte Carlo test beam event.  Figure \protect\subref{fig:reco3d} shows the \threed reconstruction output for this event where the correctly reconstructed and tagged triggered test beam particle has been highlighted.  Figures \protect\subref{fig:recou}, \protect\subref{fig:recov} and \protect\subref{fig:recow} show the \twod hits for the reconstructed test beam particle where each coloured cluster of hits represents a different particle in the reconstructed particle hierarchy.}
\label{pandora_protodune_reco} 
\end{figure}

The effect of cosmic-ray backgrounds and the test beam particle halo on the reconstructed test beam particle efficiency is illustrated in Figure~\ref{fig:pandora_protodune_tbrecoeffbrkdwn}, where the efficiency is shown as a function of the momentum of the triggered particle (\ref{fig:pandora_protodune_tbrecoeffbrkdwn_p}) and the number of hits produced by the triggered particle (\ref{fig:pandora_protodune_tbrecoeffbrkdwn_nhits}).  These figures indicate that the primary loss mechanisms in the test beam particle reconstruction, accounting for $\approx 70\%$ of all inefficiencies, are due to irreducible cosmic-ray and  beam halo backgrounds.



\begin{figure}[!ht] %Anne leaving as is 5/28
\centering
\subfloat[]{\label{fig:pandora_protodune_tbrecoeffbrkdwn_p}\includegraphics[width=0.5\textwidth]{./Pandora/ProtoDUNE/BeamParticleEfficiencyBreakdownVsMomentum.pdf}}
\subfloat[]{\label{fig:pandora_protodune_tbrecoeffbrkdwn_nhits}\includegraphics[width=0.5\textwidth]{./Pandora/ProtoDUNE/BeamParticleEfficiencyBreakdownVsNHits.pdf}} \\
\subfloat[]{\label{fig:pandora_protodune_tbrecoeffbrkdwn_evt}\includegraphics[width=0.6\textwidth]{./Pandora/ProtoDUNE/EventComposition.pdf}}
\caption[Reconstruction efficiency for test beam particle in MC per momentum and hits]{The efficiency of reconstruction for the triggered test beam particle in Monte-Carlo as a function of \protect\subref{fig:pandora_protodune_tbrecoeffbrkdwn_p} the triggered beam momenta and \protect\subref{fig:pandora_protodune_tbrecoeffbrkdwn_nhits} the number of hits made by the triggered particle.  The three curves show the reconstruction efficiency of the triggered test beam particle in isolation (black), with beam particle halo overlaid (red) and with both beam particle halo and cosmic-ray backgrounds overlaid (blue).  Figure \protect\subref{fig:pandora_protodune_tbrecoeffbrkdwn_evt} shows the W plane view for a Monte-Carlo event where the triggered beam particle is shown in blue, the beam halo in red and the cosmic-ray backgrounds in black.}
\label{fig:pandora_protodune_tbrecoeffbrkdwn}
\end{figure} 

Alongside the test beam particle reconstruction metrics, the \dword{pandora} cosmic ray reconstruction has been studied using \dword{pdsp} data. 

Figure \ref{fig:pandora_protodune_cr_n} shows the number of distinct, i.e. that contain at least 100 hits, reconstructed cosmic rays per event.  Both data and \dword{mc} have a similar average number of cosmic rays per event; $53.17\pm0.02$ for data and $54.34\pm0.06$ for simulation.  However, the \dword{mc} distribution has a larger tail suggesting differences between the cosmic-ray profile in data and that used in simulation. Figure~\ref{fig:pandora_protodune_cr_recovsmc} shows the number of matched reconstructed cosmic rays per event as a function of the number of ``target'' reconstructable (as explained in Section~\ref{sec:Pandora:assessment}) distinct cosmic rays per event for \dword{mc} simulation, illustrating that the \dword{pandora} cosmic ray reconstruction is highly efficient.  The \dword{pandora} reconstruction is also able to tag the true time that a cosmic ray passes through the detector, $t_{0}$, should it cross a drift volume boundary, either \dword{cpa} or \dword{apa}.  This allows us to compare the $t_{0}$ distribution for tagged cosmic rays in data and \dword{mc}, shown in Figure \ref{fig:pandora_protodune_cr_t}.  There is excellent agreement between data and \dword{mc} in this instance.  The peak in the data distribution at $\approx 75$~ns appears due to channels affected by a known issue with the cold electronics that is now mitigated in the latest reconstruction.

Figure~\ref{fig:pandora_protodune_cr_tres} shows the resolution on the reconstructed $t_{0}$ for \dword{mc}, which indicates that the \dword{pandora} $t_{0}$ tagging is precise to the order of microseconds.  The shift in the mean of the distribution when applying the space charge effects is due to the effect of bowing of the tracks when space charge is applied.  Furthermore, the broadening of the distribution when applying the fluid flow model, in comparison to space charge, is due to the fact that the bowing effect is no longer correlated between tracks in the asymmetric fluid flow model. The size of the space charge effect is computed from simulations and efforts are ongoing to produce a data-driven space charge simulation.

\begin{figure}[!ht] %Anne leaving as is 5/28
\centering
\subfloat[]{\label{fig:pandora_protodune_cr_n}\includegraphics[width=0.5\textwidth]{./Pandora/ProtoDUNE/NumberofReconstructedCosmicRays.pdf}}
\subfloat[]{\label{fig:pandora_protodune_cr_recovsmc}\includegraphics[width=0.357\textwidth]{./Pandora/ProtoDUNE/CRMatchesCosmicRayEvent.pdf}} \\
\caption[Reconstructed cosmic rays per event for data and MC]{The number of distinct, i.e., containing at least 100 hits, reconstructed cosmic rays per event is shown in figure \protect\subref{fig:pandora_protodune_cr_n} for data and \dword{mc}.  Figure \protect\subref{fig:pandora_protodune_cr_recovsmc} shows the number of matched reconstructed cosmic rays per event as a function of the true number of reconstructable distinct cosmic rays passing through the detector per event for \dword{mc}.}
\label{fig:pandora_protodune_cr_number}
\end{figure}

\begin{figure}[!ht]  %Anne leaving as is 5/28
\centering
\subfloat[]{\label{fig:pandora_protodune_cr_t}\includegraphics[width=0.5\textwidth]{./Pandora/ProtoDUNE/StitchedT0.pdf}}
\subfloat[]{\label{fig:pandora_protodune_cr_tres}\includegraphics[width=0.5\textwidth]{./Pandora/ProtoDUNE/CosmicRayT0Resolution.pdf}}
\caption[Distribution of reconstructed $t_{0}$ for cosmic rays]{\protect\subref{fig:pandora_protodune_cr_t} The distribution of the reconstructed $t_{0}$ for cosmic rays crossing the \dword{cpa} in both data and \dword{mc}.  \protect\subref{fig:pandora_protodune_cr_tres} the resolution on the reconstructed $t_{0}$ in \dword{mc} with different space charge effects applied; No space charge (black), space charge (red) and fluid flow (blue).}
\label{fig:pandora_protodune_cr_t0}
\end{figure}
%\subsection{Calorimetry Reconstruction in Single-Phase ProtoDUNE}
%\label{sec:caloprotodune}


\subsection{High-Level Reconstruction}
\label{sec:Pandora:High}

This section presents a series of studies to illustrate the results of current efforts on high-level reconstruction, analyzing different reconstructed quantities for tracks and showers. After the pattern recognition stage provided by \dword{pandora}, further fits to the reconstructed \threed particles can be made in order to characterize their properties. For the moment, the results presented here use only the output provided by \dword{pandora}, which includes a first pass of high-level reconstruction to build these objects. For tracks, \dword{pandora} sliding linear fits are used to calculate the trajectory of the particle, whereas for showers a principal component analysis (\dword{pca}) is used to estimate directions and opening angles. 

The opening angle between the reconstructed and the true \threed direction of tracks and showers is presented in Fig. \ref{fig:pandora_dunefd_highlevel_reco_angle} in simulated \dword{fd} neutrino events\footnote{The distributions in Figs. \ref{fig:pandora_dunefd_highlevel_reco_angle} and \ref{fig:pandora_dunefd_highlevel_reco_length} include only good reco-true matches, requiring a minimum of 10\% completeness and 50\% purity for the match. In addition, Fig.~\ref{fig:pandora_dunefd_highlevel_reco_length} is made using only contained tracks, by requiring that both true start and end point are within the fiducial volume. }. The reconstructed direction of tracks is obtained as the initial momentum of the track, after a \dword{pandora} sliding linear fit is performed to its reconstructed \threed points. For showers, the reconstructed direction corresponds to the primary eigenvector result of the \dword{pca} fit to its reconstructed \threed points. In both cases, the opening angle is very small, indicating a good agreement between the reconstructed and true direction of the particles. The few cases in which an opening angle of $~\pi$ is obtained are explained by a good reconstruction of the particle (hit clustering) but the particle vertex placed at its wrong end. 

\begin{dunefigure}
[Opening angle between reconstructed and true direction for track- and shower-like particles]
{fig:pandora_dunefd_highlevel_reco_angle}
{Distribution of opening angle (in radians) between reconstructed and true direction for track-like (left) and shower-like (right) particles in simulated \dword{fd} neutrino events.}
\includegraphics[width=0.49\textwidth]{./Pandora/MCC11_OpeningAngle_Tracks_new.png}
\includegraphics[width=0.49\textwidth]{./Pandora/MCC11_OpeningAngle_Showers_new.png}
\end{dunefigure}


For track-like particles, another quantity that can be explored in the high-level reconstruction is the length. Figure~\ref{fig:pandora_dunefd_highlevel_reco_length1} shows the difference between reconstructed and true particle length ($\Delta L$), computed as the \threed distance between start and end positions, for simulated track-like particles of various types in the \dword{fd}. The difference in length $\Delta L$ clearly depends on the particle type: for example, $\sim$90\% ($\sim$83\%) of muons have a $\Delta L$ smaller than 10 cm (5cm), whereas for protons (pions) the fraction within 5 cm is $\sim$78\% ($\sim$54\%). $\Delta L$ depends on the true length of the particle, as shown in Fig.~\ref{fig:pandora_dunefd_highlevel_reco_length2}, which presents the mean and sigma (as marker and error bar respectively) of the $\Delta L$ distribution in different ranges of true length for different particle types\footnote{A Gaussian fit is performed to the $\Delta L$ distributions in each range of true length, except the first one (true length < 10cm) which presents a larger tail and its behaviour is better represented by a Landau distribution, of which the most probable value is given instead}. In general, small values of $\Delta L$ can be understood in terms of the efficiency of 3D points creation, and resolution of the vertex reconstruction. Particles presenting kinks due to scattering, such as pions and to a lesser extent protons, have the additional risk of merging parent and daughter particles when the scattering angle is small, increasing the value of $\Delta L$. Short pions are in particular subject to this effect, in addition to merges with other close or overlapping particles in complex topologies, which might translate into larger values of $\Delta L$. 

\begin{figure}[!ht]   %Anne leaving as is 5/28
\centering
\subfloat[]{\label{fig:pandora_dunefd_highlevel_reco_length1}}\includegraphics[width=0.5\textwidth]{./Pandora/MCC11_Tracks_nu_DeltaL_50cm.png}\subfloat[]{\label{fig:pandora_dunefd_highlevel_reco_length2}}\includegraphics[width=0.5\textwidth, height=6cm]{./Pandora/MCC11_Tracks_DeltaL_vs_trueL_landau.png}
\caption[Distribution of reconstructed-true length for different track-like particles]{Distribution of reconstructed - true length (\threed distance between start and end positions) for different track-like particles \protect\subref{fig:pandora_dunefd_highlevel_reco_length1}, and mean and sigma (as marker and error bar respectively) of the $\Delta L$ distribution in different ranges of true length\protect\subref{fig:pandora_dunefd_highlevel_reco_length2},}
\label{fig:pandora_dunefd_highlevel_reco_length}
\end{figure}

A number of these variables can be also explored in experimental data taken by the \dword{pdsp} detector. %In the case of the direction, the distribution of the opening angle between the expected direction and a fit to the start of the reconstructed test beam particle in \dword{pdsp} is shown for both data and \dword{mc} in Figure~\ref{fig:pandora_protodune_openingangle}.  
For example, figure~\ref{fig:pandora_protodune_vertex} presents a measurement of the test beam particle interaction vertex by comparing the end point of the primary test beam particle track and the fitted interaction vertex for \dword{pdsp} data and \dword{mc} events. 

%\begin{figure}[!ht]
%\centering
%\includegraphics[width=0.75\textwidth]{./Pandora/ProtoDUNE/BeamParticleOpeningAngle.pdf}
%\caption[Distribution of opening angle between expected direction and fit to reconstructed test beam particle, data and \dword{mc}]{The distribution of the opening angle between the expected direction and a fit to the start of the reconstructed test beam particle for both data and \dword{mc}.  For data, the expected direction is given by the trigger, while for the \dword{mc} the truth information is used.  The discrepancy between the opening angles for data and \dword{mc}, particularly for low angles, is likely to be due to deviations in the expected direction for data as the particle travels from the trigger to the TPC active volume.}
%\label{fig:pandora_protodune_openingangle}
%\end{figure}


\begin{dunefigure}
[Resolution on test beam interaction vertex on ProtoDUNE data and MC events]
{fig:pandora_protodune_vertex}
{Resolution on the test beam particle interaction vertex on \dword{protodune} data and \dword{mc} events, calculated comparing the end point of the primary test beam particle track and the fitted interaction vertex.}
\includegraphics[width=0.75\textwidth]{./Pandora/ProtoDUNE/primaryTrackVertexDiff.pdf}
\end{dunefigure}


%Distributions of dEdx for stopping muons and protons are shown in Figures\ref{fig:pandora_protodune_mu} and \ref{fig:pandora_protodune_proton} respectively... 
Cosmic-ray muons in the \dword{pdsp} detector are also used to calibrate the detector nonuniformity and determine the absolute energy scale. Cathode crossing cosmic-ray muons with $t_{0}$ information are used to correct for the attenuation effect caused by impurities in the \lar. Stopping cosmic-ray muons are used to determine the calorimetry constants that convert the calibrated \dword{adc} counts to the number of electrons so that the $dE/dx$ versus residual range distributions match the expectation, as shown in Figures~\ref{fig:muon_dedx_resrange_run5387} and~\ref{fig:muon_dedx_resrange_sce} for \dword{pdsp} data and \dword{mc} simulation with space charge effects after calibration. The data $dE/dx$ distribution has better resolution because the purity in data is better than in the simulation. 

\begin{figure}[!ht]   %Anne leaving as is 5/28
\centering
\subfloat[]{\label{fig:muon_dedx_resrange_run5387}\includegraphics[width=0.33\textwidth]{muon_run5387_dedx.pdf}}
\subfloat[]{\label{fig:muon_dedx_resrange_sce}\includegraphics[width=0.33\textwidth]{muon_sce_mc.pdf}}
\subfloat[]{\label{fig:muon_comparison_dedx_sce_data}\includegraphics[width=0.33\textwidth]{muon_dEdx.pdf}}
\caption[Stopping muon $dE/dx$ distributions for the ProtoDUNE-SP cosmic data and MC]{Stopping muon $dE/dx$ distributions for the \dword{pdsp} cosmic data and \dword{mc}. The red curves in \protect\subref{fig:muon_dedx_resrange_run5387} and \protect\subref{fig:muon_dedx_resrange_sce} are the expected most probable value of $dE/dx$ versus residual range.}
\label{fig:pandora_protodune_mu}
\end{figure}

The same attenuation correction and calorimetry constants are applied to the beam proton data and \dword{mc} and the resulting $dE/dx$ distributions are shown in Figure~\ref{fig:pandora_protodune_proton}. The data and \dword{mc} $dE/dx$ distributions agree well. Discrepancy with expectation is observed in the large residual range region, which corresponds to the beam entering point on the TPC front face where space charge effects are large. Good progress is being made on the space charge effects calibration, which will lead to more accurate $dE/dx$ measurements.
\begin{figure}[!ht]  %Anne leaving as is 5/28
\centering
\subfloat[]{\label{fig:proton_dedx_resrange_run5387}\includegraphics[width=0.33\textwidth]{proton_dedx_resrange_run5387.pdf}}
\subfloat[]{\label{fig:proton_dedx_resrange_sce}\includegraphics[width=0.33\textwidth]{proton_dedx_resrange_sce.pdf}}
\subfloat[]{\label{fig:proton_comparison_dedx_sce_data}\includegraphics[width=0.33\textwidth]{proton_comparison_dedx_sce_data.pdf}}
\caption[Proton $dE/dx$ distributions for the ProtoDUNE-SP 1 GeV beam data and MC]{Proton $dE/dx$ distributions for the \dword{pdsp} 1 GeV beam data and \dword{mc}. The red curves in \protect\subref{fig:proton_dedx_resrange_run5387} and \protect\subref{fig:proton_dedx_resrange_sce} are the expected most probable value of $dE/dx$ vs residual range.}
\label{fig:pandora_protodune_proton}
\end{figure}


\clearpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{vol-physics/phys-tools-calibration.tex}



%\clearpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Tools and Assumptions use for Evaluation of \dword{nd} Capabilities}
%\label{sec:tools-nd-eval}
%
%\fixme{This section is intended to provide some common descriptions with application to later chapters.  For now, the relevant chapters contain sufficiently complete standalone descriptions.  We may leave it that way.}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Appendix: Tools and Methods}
\label{sec:tools-appendix}

\subsection{Additional Simulation Details}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Additional Flux Modeling Details}
\label{sec:tools-app-flx}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Additional Physics Generator Details}
\label{sec:tools-app-generator}

Building a comprehensive model for the simulation of neutrino interactions in the energy range of interest to current and near-future experiments poses significant challenges. This broad energy range bridges the perturbative and non-perturbative pictures of the nucleon and a variety of scattering mechanisms are important. In many areas, including elementary cross sections, hadronization models, and nuclear physics, one is required to piece together models with different ranges of validity in order to generate events over all of the available phase space. This inevitably introduces challenges in merging and tuning models, making sure that double counting and discontinuities are avoided. In addition, there are kinematic regimes which are outside the stated range of validity of all available models, in which case we are left with the challenge of developing our own models or deciding which model best extrapolates into this region. 

At the time of writing this document, Generator v3.0.0 was the most recent version released by the GENIE Collaboration [(1) Paper on GENIE3 in progress -- Will provide reference ASAP]. This version includes several new, state-of-the-art comprehensive models and tunes, whose performance against a large collection of neutrino, charged-lepton and hadron scattering data was characterised in detail [(2) Paper with detailed model characterization in progress -- Will provide reference ASAP]. The following comprehensive models and tunes are currently available:

{\bf G00\_00a\_00\_000} and {\bf G00\_00b\_00\_000}: They are equivalent to the historical ``Default'' and``Default+MEC'' models, as they were implemented in the latest releases of the v2 series of the GENIE Generator. They are empirical comprehensive models, based on home-grown hadronic simulations (AGKY model \cite{Yang:2009zx} for neutrino-induced hadronization and INTRANUKE/hA model \cite{Dytman:2015taa} for hadronic re-interactions) and nuclear neutrino cross-sections calculated within the framework of the simple Relativistic Fermi Gas model \cite{Bodek:1981wr}. Several processes are simulated within that framework with the most important ones, in terms of the size of the corresponding cross-section at few-GeV, being: a) quasi-elastic scattering, simulated using an implementation of the Llewellyn Smith model \cite{LlewellynSmith:1971uhs}, b) multi-nucleon interactions, simulated with an empirical model motivated by the Lightbody model \cite{Lightbody:1988gcu} and using a nucleon cluster model for the simulation of the hadronic system, c) baryon resonance neutrino-production simulated using an implementation of the Rein-Sehgal model \cite{Rein:1980wg}, and d) deep-inelastic scattering, simulated using the model of Bodek and Yang \cite{Bodek:2002ps}.  These comprehensive models, as well as the GENIE procedure for tuning the cross-section model in the transition region, have been used for several years and are well understood and documented \cite{Andreopoulos:2009rq}. The actual tune used is the one produced for the analysis of data from the MINOS experiment and, as it was already known at that time, it has several caveats as it emphasises inclusive data and does not address tensions with exclusive data. These comprehensive models and tunes are now unsupported, as they are superseded by the substantially improved versions listed below. However, they are included in v3 as they provide a connection with a large body of neutrino interaction studies.

{\bf G18\_01a\_02\_11a} and {\bf G18\_01b\_02\_11a}: They are adiabatically improved versions of the historical comprehensive models. They include new processes (diffractive production of pions, hyperon production) and major upgrades to the final-state hadronic re-interaction models (the `01a' version uses the INTRANUKE/hA2018 hadronic re-interaction model, whereas `01b' uses the INTRANUKE/hN2018 model) [(1) Paper on GENIE3 in progress], but leave the base cross-section model unchanged with respect to the historical model. However, the cross-section model was re-tuned to deuterium data and shows much improved agreement with several exclusive $1\pi$ and $2\pi$ channels, while it maintained good agreement with inclusive data. [(3) Paper on tunes in progress -- Will provide reference ASAP]

{\bf G18\_02a\_02\_11a} and {\bf G18\_02b\_02\_11a}: They are improved empirical models that share many of the features of G18\_01a\_02\_11a and G18\_01b\_02\_11a (new processes, improved hadronic simulations, new tunes), but introduced changes to the base cross-section model. In these comprehensive models, the Rein-Sehgal coherent and resonance neutrino-production models \cite{Rein:2006di, Rein:1980wg} were replaced with the better-motivated models produced by Berger-Sehgal \cite{Berger:2008xs, Berger:2007rq}.

{\bf G18\_10a\_02\_11a} and {\bf G18\_10b\_02\_11a}: They are comprehensive models anchored on the best theory currently impleented in GENIE. They are similar to G18\_02a\_02\_11a and G18\_02b\_02\_11a, respectively, but for the simulation of quasielastic-like processes they implement a modern microscopic calculation by the Valencia group. Quasi-elastic processes, within a Local Fermi Gas (LFG) model and simulating Coulomb and Random Phase Approximation (RPA) corrections are implemented based on Ref. \cite{Nieves:2004wx}, whereas 2p2h processes are implemented based on Ref. \cite{Nieves:2011pp}.

{\bf G18\_10i\_00\_000} and {\bf G18\_10j\_00\_000}: They are similar to G18\_10a\_02\_11a and G18\_10b\_02\_11a, but using the better-motivated $z$-expansion \cite{Hill:2010yb} of the axial form factor. However, due to extra complications in incorporating deuterium experiment flux constraints when using the $z$-expansion model in the GENIE tunes, both of these comprehensive models were untuned in v3.0.0, though a tune will be made available in a future revision. 
\fixme{can't find citation Hill:2010yb. Anne}
\paragraph{Nucleon Decay Channels}

The nucleon decay channels simulated by \dword{genie} are listed in Table~\ref{tab:genie_ndk}.
%while those used in the simulation of neutron-antineutron oscillations are given in 
%Table~\ref{tab:genie_antineutron}.

%\fixme{ TODO: replace raster version - was in table \ref{costas_table1} caption}

\begin{table} % Anne leaving as is
  \includegraphics[width=\linewidth]{costas_table1}
  
  \caption[GENIE nucleon decay topologies]{Decay topologies considered in \dword{genie} nucleon decay simulation.}
  \label{tab:genie_ndk}
\end{table}

%\begin{table}
%	\begin{centering}
%		\includegraphics[width=.8\linewidth]{costas_table2}\\
%    \end{centering}
%  
%  \caption[\dword{genie} antineutron annihilation topologies]{Antineutron annihilation topologies considered in the \dword{genie} neutron-antineutron oscillation simulation. TODO: replace raster version}
%  \label{tab:genie_antineutron}
%\end{table}



